# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# # Bottle Processing
# Author: Andrew Reed
# Date: 2021-01-12
#
# ### Motivation:
# Independent verification of the suite of physical and chemical observations provided by OOI are critical for the observations to be of use for scientifically valid investigations. Consequently, CTD casts and Niskin water samples are made during deployment and recovery of OOI platforms, vehicles, and instrumentation. The water samples are subsequently analyzed by independent labs for  comparison with the OOI telemetered and recovered data.
#
# However, currently the water sample data routinely collected and analyzed as part of the OOI program are not available in a standardized format which maps the different chemical analyses to the physical measurements taken at bottle closure. Our aim is to make these physical and chemical analyses of collected water samples available to the end-user in a standardized format for easy comprehension and use, while maintaining the source data files. 
#
# ### Approach:
# Generating a summary of the water sample analyses involves preprocessing and concatenating multiple data sources, and accurately matching samples with each other. To do this, I first preprocess the ctd casts to generate bottle (.btl) files using the SeaBird vendor software following the SOP available on Alfresco. 
#
# Next, the bottle files are parsed using python code and the data renamed following SeaBird's naming guide. This creates a series of individual cast summary (.sum) files. These files are then loaded into pandas dataframes, appended to each other, and exported as a csv file containing all of the bottle data in a single data file.
#
# ### Data Sources/Software:
#
# * **sbe_name_map**: This is a spreadsheet which maps the short names generated by the SeaBird SBE DataProcessing Software to the associated full names. The name mapping originates from SeaBird's SBE DataProcessing support documentation.
#
# * **Alfresco**: The Alfresco CMS for OOI at alfresco.oceanobservatories.org is the source of the ctd hex, xmlcon, and psa files necessary for generating the bottle files needed to create the sample summary sheet.
#
# * **SBEDataProcessing-Win32**: SeaBird vendor software for processing the raw ctd files and generating the .btl files.
#

import os, sys, re
import pandas as pd
import numpy as np
import datetime
import pytz
import warnings

sbe_name_map = pd.read_excel('/media/andrew/OS/Users/areed/Documents/OOI-CGSN/QAQC_Sandbox/Reference_Files/seabird_ctd_name_map.xlsx')

sbe_name_map.head()

column_order = pd.read_excel("/media/andrew/Files/Water_Sampling/column_order.xlsx")
column_order = column_order.columns

# ---
# Set the directory paths to where the relevant information is stored:

basepath = "/media/andrew/Files/Water_Sampling/"
array = "Coastal_Pioneer_Array/"
cruise = "Pioneer-02_KN217_2014-04-11/"
water_sampling = "Ship_Data/Water_Sampling/Cleaned/"
ctd = "Ship_Data/ctd/"

# ---
# ## BTL Data
# First, load in the bottle data from the CTD casts. This data should already have been processed using the SeaBird Processing software to produce the relevant .btl files.

from bottle_utils import Cast

# Get the directory where the **```.btl```** files are:

BTL_DIR = basepath + array + cruise + ctd
for file in os.listdir(BTL_DIR):
    if file.endswith(".btl"):
        print(file)

# Iterate through the directory where the bottle files are stored, parsing them into a single dataframe:

# +
Bottles = pd.DataFrame()

for file in os.listdir(BTL_DIR):
    if file.endswith(".btl"):
        # Get the cast number from the file name
        cast_no = file[file.find(".")-3:file.find(".")]
        try:
            cast_no = int(cast_no)
        except ValueError:
            cast_no = cast_no.lstrip("0")
        
        # Initialize the CTD Cast object
        cast = Cast(cast_no)
        
        # Parse the cast data
        cast.parse_cast(BTL_DIR+"/"+file)
        
        # Add in the cast number
        df = pd.DataFrame(cast.data)
        
        for key, item in cast.header.items():
            df.insert(1, key, item)

        df.insert(0, "Cast", cast.cast_number.zfill(3))
        
        # Save the results in a dataframe
        Bottles = Bottles.append(df, ignore_index=True)
        
Bottles.head()


# -

# Reformat the Start Longitude and Start Latitude to be decimal degrees and Bottle Closure Time to "YYYY-mm-ddTHH:MM:SS.000Z"

# +
def parse_start_latitude(x):
    if "S" in x:
        x = x.replace("N","").lstrip("0").strip().split()
        x = float(x[0]) + float(x[1])/60
        x = -1*x
    elif "N" in x:
        x = x.replace("N","").lstrip("0").strip().split()
        x = float(x[0]) + float(x[1])/60
    else:
        pass
    
    return x

def parse_start_longitude(x):
    if "W" in x:
        x = x.replace("N","").lstrip("0").strip().split()
        x = float(x[0]) + float(x[1])/60
        x = -1*x
    elif "E" in x:
        x = x.replace("N","").lstrip("0").strip().split()
        x = float(x[0]) + float(x[1])/60
    else:
        pass
    
    return x

Bottles["Date Time"] = Bottles["Date Time"].apply(lambda x: pd.to_datetime(x).strftime("%Y-%m-%dT%H:%M:%S.000Z"))
Bottles["Start Time [UTC]"] = Bottles["Start Time [UTC]"].apply(lambda x: pd.to_datetime(x).strftime("%Y-%m-%dT%H:%M:%S.000Z"))
Bottles["Start Latitude [degrees]"] = Bottles["Start Latitude [degrees]"].apply(lambda x: parse_start_latitude(x))
Bottles["Start Longitude [degrees]"] = Bottles["Start Longitude [degrees]"].apply(lambda x: parse_start_longitude(x))
Bottles["CStarTr0"] = Bottles["CStarTr0"].apply(lambda x: x.replace("(avg)","").strip())
Bottles["Filename"] = Bottles["Filename"].apply(lambda x: x.split("\\")[-1])

Bottles.head()
# -

# Rename the column title using the sbe_name_mapping

# +
for colname in list(Bottles.columns.values):
    try:
        fullname = list(sbe_name_map[sbe_name_map['Short Name'].apply(lambda x: str(x).lower() == colname.lower()) == True]['Full Name'])[0]
        Bottles.rename({colname:fullname},axis='columns',inplace=True)
    except:
        pass
    
Bottles.head()
# -
# Add the Cruise ID/Name to the Bottle Data

Bottles.insert(0, "Cruise", "KN217")
Bottles.head()

# +
# Try grouping the 
# -



# Save the bottle data


cruise

date = datetime.datetime.now(tz=pytz.UTC).strftime("%Y-%m-%d")
filename = f"Pioneer-03_Leg-1_KN222_CTD_Bottle_Data_{date}_ACR.xlsx"
filename

SAVE_PATH = basepath + array + cruise + water_sampling
os.listdir(SAVE_PATH)

SAVE_FILE = SAVE_PATH + filename
SAVE_FILE

Bottles.to_excel(SAVE_FILE, index=False)

# Rename the Bottle Columns with "BOT: " as a header

for col in Bottles.columns:
    Bottles.rename(columns={col: "BOT: " + col.strip()}, inplace=True)
Bottles.head()

# ---
# ## CTD Log 
#
# First, find the directory of where to find the CTD Log for the cruise:

LOG_DIR = "/".join((basepath, array, cruise, water_sampling))
for file in sorted(os.listdir(LOG_DIR)):
    if "CTD" in file:
        print(file)

# Next, set the paths to load the Log files for the cruise(s):

filename = "Pioneer-02_KN217_CTD_Sampling_Log_2021-01-06_Ver_1-01.xlsx"
Log = pd.read_excel(LOG_DIR + "/" + filename, sheet_name="Summary")
Log.head()

# Clean up headers:

# +
# Check for any carriage returns and remove
for colname in Log.columns:
    Log.rename(columns={colname: colname.replace("\n"," ")}, inplace=True)
    
# Sometimes spaces are missing before the #-symbol
for colname in Log.columns:
    Log.rename(columns={colname: colname.replace("#"," #")}, inplace=True)

# Check for extra spaces and remove
for colname in Log.columns:
    Log.rename(columns={colname: colname.replace("  ", " ")}, inplace=True)

# Check the results
Log.columns


# -

# Clean up the **station-cast #** so it is uniform type

def clean_station_num(x):
    if pd.isnull(x):
        return x
    else:
        x = str(x).zfill(3)
        return x


Log["Station-Cast #"] = Log["Station-Cast #"].apply(lambda x: clean_station_num(x))
Log["Station-Cast #"].unique()


# Clean up **Start Date** and **Start Time** into a single column

def combine_datetime(x, y):
    if pd.isnull(y):
        y = "00:00:00"
    dt = x + " " + y
    dt = pd.to_datetime(dt)
    return dt


Log["Start DateTime"] = Log[["Start Date","Start Time"]].apply(lambda x: combine_datetime(x[0], x[1]), axis=1)
Log = Log.drop(columns=["Start Date", "Start Time"])
Log.head()

# Clean up & reformat the Start Latitude, Start Longitude, and Start DateTime:

Log["Start Latitude"] = Log["Start Latitude"].apply(lambda x: parse_start_latitude(x))
Log["Start Longitude"] = Log["Start Longitude"].apply(lambda x: parse_start_longitude(x))
Log["Start DateTime"] = Log["Start DateTime"].apply(lambda x: pd.to_datetime(x).strftime("%Y-%m-%dT%H:%M:%S.000Z"))

# Rename Station-Cast and Niskin to match other columns

Log = Log.rename(columns={
    "Station-Cast #": "Station ID",
    "Niskin #": "Niskin ID"
})


# ---
# ## Discrete Bottle Sampling Data

def clean_bottle_pos(x):
    if pd.isnull(x):
        return x
    else:
        x = int(x)
        return x


Log["Niskin #"] = Log["Niskin #"].apply(lambda x: clean_bottle_pos(x))
#Bottles["Bottle Position"] = Bottles["Bottle Position"].apply(lambda x: clean_bottle_pos(x))

# ---
# ## Salinity & Oxygen Data
# Next, need to load the Salinity and Oxygen discrete data into dataframes. The data should be summarized in an excel sheet following the naming scheme ```<Cruise Name>_<OXY/SAL>_Summary.xlsx```.

# **Salinity** <br>
# Load the Salinity data

LOG_DIR = "/".join((basepath, array, cruise, water_sampling))
for file in sorted(os.listdir(LOG_DIR)):
    if "Sal" in file:
        print(file)

filename = "Pioneer-02_KN217_Salinity_Sample_Data_2021-01-06_Ver_1-00.xlsx"
SAL_FILE = basepath + array + cruise + water_sampling + filename

Salinity = pd.read_excel(SAL_FILE, sheet_name="Salinity")
Salinity.head()

# **Oxygen** <br>
# Load the Oxygen data

LOG_DIR = "/".join((basepath, array, cruise, water_sampling))
for file in sorted(os.listdir(LOG_DIR)):
    if "Oxy" in file:
        print(file)

filename = "Pioneer-02_KN217_Oxygen_Sample_Data_2021-01-06_Ver_1-00.xlsx"
OXY_FILE = basepath + array + cruise + water_sampling + filename

Oxygen = pd.read_excel(OXY_FILE, sheet_name="Oxygen")
Oxygen.head()

# ---
# ## Nutrients Data
# Sometimes the nutrient data needs to be 

LOG_DIR = "/".join((basepath, array, cruise, water_sampling))
for file in sorted(os.listdir(LOG_DIR)):
    if "Nut" in file:
        print(file)

filename = "Pioneer-02_KN217_Nutrients_Sample_Data_2020-12-30_Ver_1-01.xlsx"
NUT_FILE = basepath + array + cruise + water_sampling + filename

Nutrients = pd.read_excel(NUT_FILE)#, sheet_name="Summary")
Nutrients.head()

# ---
# ## Chlorophyll Data

LOG_DIR = "/".join((basepath, array, cruise, water_sampling))
for file in sorted(os.listdir(LOG_DIR)):
    if "Chl" in file:
        print(file)

filename = "Pioneer-02_KN217_Chlorophyll_Sample_Data_2020-03-30_ver_1-01.xlsx"
CHL_FILE = basepath + array + cruise + water_sampling + filename

Chlorophyll = pd.read_excel(CHL_FILE)#, sheet_name="chl")

Chlorophyll.columns

# Get only the relevant Chlorophyll data
columns = ["Cruise","Cast", "Niskin", "Chl ug_per_L", "Phaeo ug_per_L", "Comments"]
Chlorophyll = Chlorophyll[columns]
Chlorophyll.head()

# ## DIC

LOG_DIR = "/".join((basepath, array, cruise, water_sampling))
for file in sorted(os.listdir(LOG_DIR)):
    if "DIC" in file:
        print(file)

filename = "Pioneer-02_KN217_DIC_Sample_Data_2015-10-29_ver_1-00.xlsx"
DIC_FILE = basepath + array + cruise + water_sampling + filename

DIC = pd.read_excel(DIC_FILE, sheet_name="OCADS")#, header=1)

DIC.columns

columns = ["CRUISE_ID", "CAST_NO", "NISKIN_NO", 'DIC_UMOL_KG', 'DIC_FLAG_W', 'TA_UMOL_KG', 'TA_FLAG_W', 'PH_TOT_MEA', 'TMP_PH_DEG_C', 'PH_FLAG_W']
DIC = DIC[columns]
DIC.head()

# # MERGE SHIT

# Merge oxygen and salinity
Oxygen.head()

Salinity.head()

Discrete = Oxygen.merge(Salinity,
                        left_on=["Cruise ID", "Station ID", "Niskin ID"],
                        right_on=["Cruise ID", "Station ID", "Niskin ID"],
                        how="outer")
drop_cols = ["Sample ID_x", "Sample ID_y"]
Discrete = Discrete.drop(columns=drop_cols)
Discrete.head()

# Merge the Discrete Oxygen and Salinity with the Nutrients
Discrete = Discrete.merge(Nutrients,
                          left_on=["Cruise ID", "Station ID", "Niskin ID"],
                          right_on=["Cruise ID", "Station ID", "Niskin ID"],
                          how="outer")
drop_cols = ["Sample ID"]
Discrete = Discrete.drop(columns=drop_cols)
Discrete.head()

# +
# Merge the Discrete data with the Chlorophyll
Chlorophyll = Chlorophyll.rename(columns={
    "Cruise": "Cruise ID",
    "Cast": "Station ID",
    "Niskin": "Niskin ID",
    "Comments": "Chl Comments"
})

Discrete = Discrete.merge(Chlorophyll,
                          left_on=["Cruise ID", "Station ID", "Niskin ID"],
                          right_on=["Cruise ID", "Station ID", "Niskin ID"],
                          how="outer")
Discrete.head()

# +
# Merge the DIC
DIC = DIC.rename(columns={
    "CRUISE_ID": "Cruise ID",
    "CAST_NO": "Station ID",
    "NISKIN_NO": "Niskin ID",
})

Discrete = Discrete.merge(DIC,
                          left_on=["Cruise ID", "Station ID", "Niskin ID"],
                          right_on=["Cruise ID", "Station ID", "Niskin ID"],
                          how="outer")
Discrete.head()


# -

# Try groupby - explode to get unique values for each Cruise - Station - Niskin

def unique_non_null(s):
    
    x = s.dropna()
    if len(x) == 0:
        return np.nan
    else:
        return x.unique()


Discrete = Discrete.groupby(by=["Cruise ID", "Station ID", "Niskin ID"]).agg(unique_non_null)

Discrete = Discrete.reset_index()
Discrete

Bottles.head()

Log.head()

Discrete["Station ID"] = Discrete["Station ID"].apply(lambda x: str(int(x)).zfill(3))
Discrete.head()

# +
# Get the union of all Stations and Niskin values
merge_cols = ["Cruise ID", "Station ID", "Niskin ID"]
Summary = pd.merge(Bottles[merge_cols],
                   Log[merge_cols],
                   left_on=merge_cols,
                   right_on=merge_cols,
                   how="outer")

Summary = Summary.merge(Discrete[merge_cols],
                        left_on=merge_cols,
                        right_on=merge_cols,
                        how="outer")
Summary
# -

# Merge the Bottle Data
Bottles.head()

Summary = Summary.merge(Bottles,
                        left_on=merge_cols,
                        right_on=merge_cols,
                        how="outer")
Summary.head()

Summary.tail()

# Merge the Log Data
Summary = Summary.merge(Discrete,
                        left_on=merge_cols,
                        right_on=merge_cols,
                        how="left")

Summary.head()

Summary.tail()



# +
# Merge the Metadata
metadata_columns = ["Cruise ID", "Station ID", "Target Station", "Start Latitude", "Start Longitude",
                    "Bottom Depth [m]", "Start DateTime"]

metadata = Log[metadata_columns]

metadata = metadata.groupby(by=["Cruise ID", "Station ID"]).agg(unique_non_null).reset_index()

metadata.head()
# -

Summary = Summary.merge(metadata,
                        left_on=["Cruise ID", "Station ID"],
                        right_on=["Cruise ID", "Station ID"],
                        how="outer")
Summary.head()

# Fill in the 
Summary["Start Time [UTC]"] = Summary["Start Time [UTC]"].fillna(Summary["Start DateTime"])
Summary["Start Longitude [degrees]"] = Summary["Start Longitude [degrees]"].fillna(Summary["Start Longitude"])
Summary["Start Latitude [degrees]"] = Summary["Start Latitude [degrees]"].fillna(Summary["Start Latitude"])

Summary.head()

Summary.tail()







# ### Merge the Bottle and Discrete Data

Bottles = Bottles.rename(columns={
    "Cruise": "Cruise ID",
    "Cast": "Station ID",
    "Bottle Position": "Niskin ID"
})
Bottles["Niskin ID"] = Bottles["Niskin ID"].apply(lambda x: int(x))
Bottles.head()

Discrete["Station ID"] = Discrete["Station ID"].apply(lambda x: str(int(x)).zfill(3))
Discrete.head()

Summary = Bottles.merge(Discrete,
                        left_on=["Cruise ID", "Station ID", "Niskin ID"],
                        right_on=["Cruise ID", "Station ID", "Niskin ID"],
                        how="outer")
Summary

Summary.to_excel(basepath + array + cruise + water_sampling + "Summary.xlsx", index=False)

Bottles.head()

Log.head()







# ## Metadata
# Next, add on the metadata from the Log info

Log.head()

Comments = Log[["Cruise ID", "Station-Cast #", "Niskin #", "Comments"]]
Comments = Comments.rename(columns={
    "Station-Cast #": "Station ID",
    "Niskin #": "Niskin ID"
})
Summary = Summary.merge(Comments,
                        left_on=["Cruise ID", "Station ID", "Niskin ID"],
                        right_on=["Cruise ID", "Station ID", "Niskin ID"],
                        how="outer")

# +
metadata_columns = ["Cruise ID", "Station-Cast #", "Target Station", "Start Latitude", "Start Longitude",
                    "Bottom Depth [m]", "Start DateTime"]

metadata = Log[metadata_columns]
metadata


# -

def unique_non_null(s):
    x = s.dropna().unique()
    if len(x) == 0:
        x = np.nan
    return x


metadata = metadata.groupby(by=["Cruise ID", "Station-Cast #"]).agg(unique_non_null).reset_index()
metadata = metadata.rename(columns={
    "Station-Cast #": "Station ID"
})
metadata.head()

Summary = Summary.merge(metadata,
                        left_on=["Cruise ID", "Station ID"],
                        right_on=["Cruise ID", "Station ID"],
                        how="outer")
Summary

# Fill in the 
Summary["Start Time [UTC]"] = Summary["Start Time [UTC]"].fillna(Summary["Start DateTime"])
Summary["Start Longitude [degrees]"] = Summary["Start Longitude [degrees]"].fillna(Summary["Start Longitude"])
Summary["Start Latitude [degrees]"] = Summary["Start Latitude [degrees]"].fillna(Summary["Start Latitude"])

# Add in other missing data
Log.columns

name_map = {
    "Cruise": "Cruise ID",
    "Station": "Station ID",
    "Target Asset": "Target Station",
    "Start Latitude [degrees]": "Start Latitude [degrees]",
    "Start Longitude [degrees]": "Start Longitude [degrees]",
    "Start Time [UTC]": "Date Time",
    "Cast": "Station ID",
    "Cast Flag": None,
    "Bottom Depth at Start Position [m]": "Bottom Depth [m]",
    "CTD File": "Filename",
    "CTD File Flag": None,
    "Niskin/Bottle Position": "Niskin ID",
    "Niskin Flag": None,
    "CTD Bottle Closure Time [UTC]": "Date Time",
    "CTD Pressure [db]": "Pressure, Digiquartz [db]",
    "CTD Pressure Flag": None,
    "CTD Depth [m]": "Depth [salt water, m]",
    "CTD Latitude [deg]": "Latitude [deg]",
    "CTD Longitude [deg]": "Longitude [deg]",
    "CTD Temperature 1 [deg C]": "Temperature [ITS-90, deg C]",
    "CTD Temperature 1 Flag": None,
    "CTD Temperature 2 [deg C]": "Temperature, 2 [ITS-90, deg C]" ,
    "CTD Temperature 2 Flag": None,
    "CTD Conductivity 1 [S/m]": "Conductivity [S/m]",
    "CTD Conductivity 1 Flag": None,
    "CTD Conductivity 2 [S/m]":"Conductivity, 2 [S/m]",
    "CTD Conductivity 2 Flag": None,
    "CTD Salinity 1 [psu]": "Salinity, Practical [PSU]",
    "CTD Salinity 2 [psu]": "Salinity, Practical, 2 [PSU]",
    "CTD Oxygen, [mL/L]": "Oxygen, SBE 43 [ml/l]",
    "CTD Oxygen Flag": None,
    "CTD Oxygen Saturation [mL/L]": "Oxygen Saturation, Garcia & Gordon [ml/l]",
    "CTD Fluorescence [mg/m^3]": None,
    "CTD Fluorescence Flag": None,
    "CTD Beam Attenuation [1/m]": "Beam Attenuation, WET Labs C-Star [1/m]",
    "CTD Beam Transmission [%]": "Beam Transmission, WET Labs C-Star [%]",
    "CTD Transmissometer Flag": None,
    "CTD pH": None,
    "CTD pH Flag": None,
    "Discrete Oxygen [mL/L]": "Oxygen [mL/L]",
    "Discrete Oxygen Flag": None,
    "Discrete Oxygen Duplicate Flag": None,
    "Discrete Chlorophyll [ug/L]": "Chl ug_per_L",
    "Discrete Phaeopigment [ug/L]": "Phaeo ug_per_L",
    "Discrete Fo/Fa Ratio": None,
    "Discrete Fluorescence Flag": None,
    "Discrete Fluorescence Duplicate Flag": None,
    "Discrete Phosphate [uM]": "Phosphate [µmol/L]",
    "Discrete Silicate [uM]": "Silicate [µmol/L]",
    "Discrete Nitrate [uM]": "Nitrate [µmol/L]", #Nitrate+Nitrite [µmol/L]
    "Discrete Nitrite [uM]": "Nitrite [µmol/L]",
    "Discrete Ammonium [uM]": "Ammonium [µmol/L]",
    "Discrete Nutrients Flag": None,
    "Discrete Nutrients Replicate Flag": None,
    "Discrete Salinity [psu]": "Salinity [psu]",
    "Discrete Salinity Flag": None,
    "Discrete Salinity Replicate Flag": None,
    "Discrete Alkalinity [umol/kg]": "TA_UMOL_KG",
    "Discrete Alkalinity Flag": "TA_FLAG_W",
    "Discrete Alkalinity Replicate Flag": None,
    "Discrete DIC [umol/kg]": "DIC_UMOL_KG",
    "Discrete DIC Flag": "DIC_FLAG_W",
    "Discrete DIC Replicate Flag": None,
    "Discrete pCO2 [uatm]": None,
    "Discrete pCO2 Analysis Temp [C]": None,
    "Discrete pCO2 Flag": None,
    "Discrete pH [Total scale]": "PH_TOT_MEA",
    "Discrete pH Analysis Temp [C]": "TMP_PH_DEG_C",
    "Discrete pH Flag": "PH_FLAG_W",
    "Discrete pH Replicate Flag": None,
    "Calculated Alkalinity [umol/kg]": None,
    "Calculated DIC [umol/kg]": None,
    "Calculated pCO2 [uatm]": None,
    "Calculated pH": None,
    "Calculated CO2aq [umol/kg]": None,
    "Calculated Bicarb [umol/kg]": None,
    "Calculated CO3 [umol/kg]": None,
    "Calculated Omega-C": None,
    "Calculated Omega-A": None,  
    "Comments": "Comments",
    "Chlorophyll Comments": "Chl Comments"
}

Final = pd.DataFrame(columns=name_map.keys())
Final

for key in name_map.keys():
    if name_map.get(key) is None:
        pass
    else:
        Final[key] = Summary[name_map.get(key)]

Final

# Determine duplicates
Final["Duplicates"] = Final.duplicated(subset=["Cruise", "Station", "Niskin/Bottle Position"], keep=False)

# Fill NaNs with -9999999
Final = Final.fillna(value=-9999999)

filename = basepath + array + cruise + water_sampling + "Pioneer-04_AT27A_Discrete_Sample_Summary_2021-01-19_ACR.xlsx"
filename

Final.to_excel(filename, index=False)

# ## Merge the Bottle and Log Data

for col in Bottles.columns:
    Bottles.rename(columns={col: "BOT: " + col.strip()}, inplace=True)
Bottles.head()

for col in Log.columns:
    Log.rename(columns={col: "LOG: " + col.strip()}, inplace=True)
Log.head()

Bot_Log = Bottles.merge(Log,
                        left_on=["BOT: Cruise ID", "BOT: Cast", "BOT: Bottle Position"],
                        right_on=["LOG: Cruise ID", "LOG: Station-Cast #", "LOG: Niskin #"],
                        how="outer")
Bot_Log.head()

Bot_Log.columns

# Fill in missing data
Bot_Log["BOT: Cruise ID"] = Bot_Log["BOT: Cruise ID"].fillna(value=Bot_Log["LOG: Cruise ID"])
Bot_Log["BOT: Cast"] = Bot_Log["BOT: Cast"].fillna(value=Bot_Log["LOG: Station-Cast #"])
Bot_Log["BOT: Bottle Position"] = Bot_Log["BOT: Bottle Position"].fillna(value=Bot_Log["LOG: Niskin #"])
Bot_Log['BOT: Start Time [UTC]'] = Bot_Log['BOT: Start Time [UTC]'].fillna(value=Bot_Log["LOG: Start DateTime"])
Bot_Log['BOT: Start Longitude [degrees]'] = Bot_Log["BOT: Start Longitude [degrees]"].fillna(value=Bot_Log["LOG: Start Longitude"])
Bot_Log['BOT: Start Latitude [degrees]'] = Bot_Log["BOT: Start Latitude [degrees]"].fillna(value=Bot_Log["LOG: Start Latitude"])

# Drop some columns
drop_cols = ["LOG: Cruise ID", "LOG: Station-Cast #", "LOG: Niskin #", "LOG: Start DateTime",
             "LOG: Start Latitude", "LOG: Start Longitude", "LOG: Start Date", "LOG: Start Time"]
Bot_Log = Bot_Log.drop(columns = drop_cols)
Bot_Log

# ## Merge the Salinity





# ## Union of Casts and Niskins
#
# First, want to get the combination of all Casts and Niskin Bottle #s for the cruise. 

# ## Begin Creating the Summary Sheet

Summary["Cruise"] = casts_niskins["Cruise ID"]
Summary["Station"] = casts_niskins["Cast"]
Summary["Niskin/Bottle Position"] = casts_niskins["Niskin"]

# ### Merge the Bottle Data to the KEYs

for col in Bottles.columns:
    Bottles.rename(columns={col: "BOT: " + col}, inplace=True)
Bottles.columns

Summary = Summary.merge(Bottles,
                        left_on=["Cruise", "Station", "Niskin/Bottle Position"],
                        right_on=["BOT: Cruise ID", "BOT: Cast", "BOT: Bottle Position"],
                        how="outer")
Summary.head()

Summary.shape

Summary.columns

# +
# Fill in the relevant columns based on the merge
# -

Summary["Cruise"] = Summary["Cruise"].fillna(Summary["BOT: Cruise ID"])
Summary["Station"] = Summary["Station"].fillna(Summary["BOT: Cast"])
Summary[""]

# ### Merge the LOG Data

for col in Log.columns:
    Log.rename(columns={col: "LOG: " + col.strip()}, inplace=True)
Log.columns

Log["LOG: Cruise ID"].unique(), Log["LOG: Station-Cast #"].unique(), Log["LOG: Rosette Position"].unique()

Log["LOG: Rosette Position"].unique()

Summary = Summary.merge(Log,
                        left_on=["KEY: Cruise ID", "KEY: Cast", "KEY: Niskin"],
                        right_on=["LOG: Cruise ID", "LOG: Station-Cast #", "LOG: Niskin #"],
                        how="left")

Summary.head()

Summary.columns

# ### Merge the Discrete Oxygen

for col in Oxygen.columns:
    Oxygen.rename(columns={col: "OXY: " + col}, inplace=True)
Oxygen.columns

Oxygen["OXY: Station ID"] = Oxygen["OXY: Station ID"].apply(lambda x: str(x).zfill(3))
Oxygen["OXY: Station ID"].unique()

Summary = Summary.merge(Oxygen,
                        left_on=["KEY: Cruise ID", "KEY: Cast", "KEY: Niskin"],
                        right_on=["OXY: Cruise ID", "OXY: Station ID", "OXY: Niskin ID"],
                        how="outer")

Summary.head()

Summary.columns

# ### Merge the Salinity

for col in Salinity.columns:
    Salinity.rename(columns={col: "SAL: " + col}, inplace=True)
Salinity.columns

Salinity["SAL: Station ID"] = Salinity["SAL: Station ID"].apply(lambda x: str(x).zfill(3))
Salinity["SAL: Station ID"].unique()

Summary = Summary.merge(Salinity,
                        left_on=["KEY: Cruise ID", "KEY: Cast", "KEY: Niskin"],
                        right_on=["SAL: Cruise ID", "SAL: Station ID", "SAL: Niskin ID"],
                        how="outer")

Summary.head()

# ### Merge the Nutrients

for col in Nutrients.columns:
    Nutrients.rename(columns={col: "NUT: " + col}, inplace=True)
Nutrients.columns

Nutrients["NUT: Station ID"] = Nutrients["NUT: Station ID"].apply(lambda x: str(x).zfill(3))
Nutrients["NUT: Station ID"].unique()

Summary = Summary.merge(Nutrients,
                        left_on=["KEY: Cruise ID", "KEY: Cast", "KEY: Niskin"],
                        right_on=["NUT: Cruise ID", "NUT: Station ID", "NUT: Niskin ID"],
                        how="outer")

Summary.head()

Summary.columns

# ### Merge the Chlorophyll Data

for col in Chlorophyll.columns:
    Chlorophyll.rename(columns={col: "CHL: " + col}, inplace=True)
Chlorophyll.columns

Chlorophyll["CHL: Cast"] = Chlorophyll["CHL: Cast"].apply(lambda x: str(x).zfill(3))
Chlorophyll["CHL: Cast"].unique()

Summary = Summary.merge(Chlorophyll,
                        left_on=["KEY: Cruise ID", "KEY: Cast", "KEY: Niskin"],
                        right_on=["CHL: Cruise", "CHL: Cast", "CHL: Niskin"],
                        how="outer")

Summary.head()

Summary.columns

# ### Merge the DIC

for col in DIC.columns:
    DIC.rename(columns={col: "DIC: " + col}, inplace=True)
DIC.columns

DIC["DIC: CAST_NO"] = DIC["DIC: CAST_NO"].apply(lambda x: str(x).zfill(3))
DIC["DIC: CAST_NO"].unique()

Summary = Summary.merge(DIC,
                        left_on=["KEY: Cruise ID", "KEY: Cast", "KEY: Niskin"],
                        right_on=["DIC: CRUISE_ID", "DIC: CAST_NO", "DIC: NISKIN_NO"],
                        how="outer")

Summary

Summary[(Summary["KEY: Cast"] == "005") & (Summary["KEY: Niskin"] == 5)]

# ### Test out  way to explode dataframe

df = pd.DataFrame({
    "Cruise ID": ["AT26-30", "AT26-30", "AT26-30", "AT26-30"],
    "Cast": [1, 1, 2, 2],
    "Salinity": [34.251, 34.302, [35.212, 35.260], [35.011, 35.065]],
    "Oxygen": [[8.512, 8.505], 8.210, [6.740, 6.755], [7.602, 7.598]]
})

df

df = df.set_index(keys=["Cruise ID", "Cast"])
df

sal = pd.DataFrame(df["Salinity"].explode())
oxy = pd.DataFrame(df["Oxygen"].explode())
oxy

df2 = pd.concat([sal, oxy])
df2





def explode_multiple(x):
    """This will use the prev func, 
       explode each columns and concat them to a dataframe"""
    m=x.applymap(lambda x: [*split_lists(x,2)])
    m=pd.concat([m.explode(i).loc[:,i] for i in m.columns],axis=1).reset_index()
    return m


explode_multiple(df)

explode_cols = ["Salinity", "Oxygen"]
for col in explode_cols:
    df = df.explode(col)
df

pd.DataFrame(df.groupby(by=["Cruise ID", "Cast", "Salinity"])).agg(np.unique).reset_index()

df = df.reset_index(drop=True)
df

df = df.set_index(keys=["Cruise ID", "Cast"], append=True)
df

pd.DataFrame(df.stack()).unstack()

new_df = pd.DataFrame(df.stack().explode())
new_df

new_df.unstack(-1)

df = df.explode("Salinity")
df

df = df.expl



# Split the columns
sal = pd.DataFrame(df["Salinity"].explode())
oxy = pd.DataFrame(df["Oxygen"].explode())
oxy



# Now, merge the data
df2 = sal.merge(oxy, left_index=True, right_index=True)
df2

dupes = df2.index.duplicated()
dupes

df2.reset_index(drop=True, inplace=True)

for ind in df2[dupes]["Oxygen"].index:
    df2["Oxygen"].loc[ind] = np.nan
df2

df3 = df2.explode("Oxygen")
df3

dupes = df3.index.duplicated()
dupes

df3.reset_index(drop=True, inplace=True)
df3

for ind in df3[dupes]["Salinity"].index:
    df3["Salinity"].loc[ind] = np.nan
df3

for col in df.columns:
    df = df.explode(col)
df

df.duplicated()


def unique_non_null(s):
    
    x = np.unique(s.dropna())
    if len(x) == 0:
        return np.nan
    else:
        return x


df.groupby(by=["Cruise ID", "Cast"]).agg(np.unique)


def multi_explode(df, lst_cols, fill_value=np.nan):
    # make sure `lst_cols` is a list
    if lst_cols and not isinstance(lst_cols, list):
        lst_cols = [lst_cols]
    # all columns except `lst_cols`
    idx_cols = df.columns.difference(lst_cols)

    # calculate lengths of lists
    lens = df[lst_cols[0]].str.len()
    lens = lens.fillna(0)

    if (lens > 0).all():
        # ALL lists in cells aren't empty
        return pd.DataFrame({
            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())
            for col in idx_cols
        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \
          .loc[:, df.columns]
    else:
        # at least one list in cells is empty
        return pd.DataFrame({
            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())
            for col in idx_cols
        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \
          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \
          .loc[:, df.columns]


def explode(df, columns):
    idx = np.repeat(df.index, df[columns[0]].str.len().fillna(1))
    a = df.T.reindex(columns).values
    concat = np.concatenate([np.concatenate(a[i]) for i in range(a.shape[0])])
    p = pd.DataFrame(concat.reshape(a.shape[0], -1).T, idx, columns)
    return pd.concat([df.drop(columns, axis=1), p], axis=1).reset_index(drop=True)


# +
explode_columns = ["Salinity", "Oxygen"]
for col in explode_columns:
    df = df.explode(col)
    
group_columns = df.columns.difference(explode_columns)
df = df.groupby(by=group_columns).agg(unique_non_null)
df
# -

list(group_columns)

df.groupby(by=list(group_columns)).agg(unique_non_null)

df.apply(lambda x: x.explode() if x.name in ["Salinity", "Oxygen"] else x)

# ## Summary Spreadsheet

# ### 1: Initialize the Summary Spreadsheet

Summary = pd.DataFrame(columns=column_order)
Summary.head()

Summary.columns

# ### 2: Add in the unique Cruise, Station, and Niskin Values

casts_niskins = casts_niskins.rename(columns={"Cruise ID": "Cruise", "Cast": "Station", "Niskin": "Niskin/Bottle Position"})
Summary = Summary.append(casts_niskins, ignore_index=True)
Summary.head()

# ### 3: Add Log Metadata
# Next, need to add in the metaata from the CTD Log. This includes the following columns: <br>
#
#     | Target Station | Start Latitude | Start Longitude | Start DateTime | Bottom Depth [m] |
#     | -------------- | -------------- | --------------- | -------------- | ---------------- |
#     | (str)          | (str)          | (str)           | (DateTime)     | (int)            |
#

metadata_columns = ["Station-Cast #","Target Station", "Start Latitude", "Start Longitude", "Start DateTime", "Bottom Depth [m]"]

metadata = Log[metadata_columns]
metadata = metadata.groupby(by="Station-Cast #").agg(np.unique)
metadata = metadata.reset_index()
metadata

Summary = Summary.merge(metadata, how="left", left_on="Station", right_on="Station-Cast #")

Summary["Target Asset"] = Summary["Target Asset"].combine_first(Summary["Target Station"])
Summary["Start Latitude [degrees]"] = Summary["Start Latitude [degrees]"].combine_first(Summary["Start Latitude"])
Summary["Start Longitude [degrees]"] = Summary["Start Longitude [degrees]"].combine_first(Summary["Start Longitude"])
Summary["Start Time [UTC]"] = Summary["Start Time [UTC]"].combine_first(Summary["Start DateTime"])
Summary["Bottom Depth at Start Position [m]"] = Summary["Bottom Depth at Start Position [m]"].combine_first(Summary["Bottom Depth [m]"])

# Drop the extra columns from the merged data

Summary.drop(columns=metadata_columns, inplace=True)
Summary.head()

# ### 4: Add Oxygen and Salinity Data

Oxygen["Station ID"] = Oxygen["Station ID"].apply(lambda x: str(x).zfill(3))
Oxygen.head()

# +
Summary_keys = ["Station","Niskin/Bottle Position"]
Oxygen_keys = ["Station ID", "Niskin ID"]

Summary = Summary.merge(Oxygen, how="left", left_on=Summary_keys, right_on=Oxygen_keys)
# -

Summary["Discrete Oxygen [mL/L]"] = Summary["Oxygen [mL/L]"]

# Drop the oxygen columns

Summary.drop(columns=Oxygen.columns, inplace=True)

# Salinity

Salinity["Station ID"] = Salinity["Station ID"].apply(lambda x: str(x).zfill(3))
Salinity.head()

# +
Summary_keys = ["Station","Niskin/Bottle Position"]
Salinity_keys = ["Station ID", "Niskin ID"]

Summary = Summary.merge(Salinity, how="left", left_on=Summary_keys, right_on=Salinity_keys)
# -

Summary["Discrete Salinity [psu]"] = Summary["Salinity [psu]"]

Summary.drop(columns=Salinity.columns, inplace=True)

# ## Add Chlorophyll Data

Chlorophyll["Cast"] = Chlorophyll["Cast"].apply(lambda x: str(x).zfill(3))
Chlorophyll.rename(columns={"Cast":"Cast_y"}, inplace=True)

Chlorophyll_keys = ["Cast_y", "Niskin"]

Summary = Summary.merge(Chlorophyll, how="left", left_on=Summary_keys, right_on=Chlorophyll_keys)
Summary

Summary["Discrete Chlorophyll [ug/L]"] = Summary["Chl ug_per_L"]
Summary["Discrete Phaeopigment [ug/L]"] = Summary["Phaeo ug_per_L"]
Summary["Chlorophyll Comments"] = Summary["Comments"]
Summary.drop(columns=Chlorophyll.columns, inplace=True)

# ## Merge the CTD Data
# Next, we want to merge the CTD onto the Station/Casts

# +
Summary_keys = ["Station", "Niskin/Bottle Position"]
Bottles_keys = ["BOT: Cast", "BOT: Bottle Position"]

# Merge the Summary and CTD Bottle Data
Summary = Summary.merge(Bottles, how="left", left_on=Summary_keys, right_on=Bottles_keys)

# Drop the Bottle Cast and Bottle Position Headers
Summary.drop(columns=["BOT: Bottle Position"], inplace=True)
Summary.head()
# -

# ## Merge the LOG Data

# +
Summary_keys = ["Station", "Niskin/Bottle Position"]
Log_keys = ["LOG: Station-Cast #", "LOG: Niskin #"]

Summary.merge(Log, how="left", left_on=Summary_keys, right_on=Log_keys)
# -



Log.columns

# ### Duplicate Samples
# Next, want to identify where there are duplicate samples for salts, oxygen, chlorophyll, and nutrients. The process is to:
# 1. Map duplicate samples into a list
# 2. Split the list into rows so each row has an unique entry from the list, copying all other column values
# 3. Replace the values
# 4. Create a new boolean column indicating the columns copied for each water property sample 

# #### 1. Split

fuck me

# +
# First, convert multiple entries into a list
Log["Log: Oxygen Bottle #"] = Log["Log: Oxygen Bottle #"].apply(lambda x: x.replace(" ","").split(",") if type(x) == str else x)

# Next, split the multiple samples into their own row
Log = Log.explode("Log: Oxygen Bottle #")
# -

# #### 2. Apply
# Now, take the oxygen data and merge it into the Water Sampling Summary Sheet based on the **Cast, Niskin, Bottle Sample ID**

for colname in Oxygen:
    Oxygen.rename(columns={colname: "Oxy: " + colname}, inplace=True)
Oxygen

Oxygen.columns

pd.merge(Log, Oxygen, how="left", left_on=["Log: Station-Cast #", "Log: Niskin #", "Log: Oxygen Bottle #"], right_on=["Oxy: Station ID", "Oxy: Niskin ID", "Oxy: Sample ID"])

# Merge the Data
Log.merge(Oxygen, how="left", left_on=[["Log: Station-Cast #", "Log: Niskin #", "Log: Oxygen Bottle #"]], right_on=[["Oxy: Station ID", "Oxy: Niskin ID", "Oxy: Sample ID"]])

Log[["Log: Station-Cast #", "Log: Niskin #", "Log: Oxygen Bottle #"]]

Oxygen[['Oxy: Station ID', 'Oxy: Niskin ID', 'Oxy: Sample ID']]







# **===================================================================================================================**
# ### Load the Sampling Log 

# Load the CTD Log csv
log = pd.read_excel(log_path, sheet_name='Summary')
# Add "Log" to the column name
for colname in list(log.columns.values):
    log.rename(columns={colname:'Log: '+colname}, inplace=True)
log.rename(columns={'Log:  Oxygen Bottle #':'Log: Oxygen Bottle #'}, inplace=True)

log.head()

log["Log: Start Time"] = log["Log: Start Time"].apply(str)

log["Log: Start Time"] = log["Log: Start Time"].apply(lambda x: pd.to_datetime(x,format='%H%M') if type(x) == str else x)
# Next, need to reformat the Date and Time columns to be pandas datetime objects and merge into a single column
log['Log: Start Date'] = log['Log: Start Date'].apply(lambda x: pd.to_datetime(x).strftime('%Y-%m-%d') if not pd.isnull(x) else '')
log['Log: Start Time'] = log['Log: Start Time'].apply(lambda x: x.strftime('%H:%M:%SZ') if not pd.isnull(x) else '00:00:00Z')
# Now combine the two columns into a single column
log['Log: Start Date'] = log['Log: Start Date'] + 'T' + log['Log: Start Time']
# Check if any start with "T" indicating no date - will replace with ''
log['Log: Start Date'] = log['Log: Start Date'].apply(lambda x: '' if x.startswith('T') else x)
# Drop the "Start Time" column because it is not needed
log.drop(columns='Log: Start Time', inplace=True)

# Now get the cruise id
cruise_id = log[log["Log: Cruise ID"].notna()]["Log: Cruise ID"].unique()[0]
cruise_id

# **If there is no sampling log (but ctd casts)**:

columns = ['Log: Cruise ID', 'Log: Station-Cast #', 'Log: Target Asset', 'Log: Start Latitude', 
           'Log: Start Longitude', 'Log: Start Date', 'Log: Start Time', 'Log: Bottom Depth [m]', 
           'Log: Date', 'Log: Niskin #', 'Log: Time', 'Log: Trip Depth', 'Log: Oxygen Bottle #',
           'Log: Ph Bottle #', 'Log: DIC/TA Bottle #', 'Log: Salts Bottle #', 'Log: Nitrate Bottle 1',
           'Log: Chlorophyll Brown Bottle #', 'Log: Chlorophyll Filter Sample #', 
           'Log: Chlorophyll Brown Bottle Volume', 'Log: Chlorophyll LN Tube', 'Log: Comments']

log = pd.DataFrame(columns=columns)

# Salts Duplicates:

# Next, we need to turn any salts duplicates into a list, map the list into rows with unique entries, then identify
# those rows which were copied
log['Log: Salts Bottle #'] = log['Log: Salts Bottle #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Salts Bottle #')
# Identify where the indices where duplicated, indicating copying
log["Log: Salts Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Salts Duplicate"]]

# Oxygen Duplicates:

log['Log: Oxygen Bottle #'] = log['Log: Oxygen Bottle #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Oxygen Bottle #')
# Identify where the indices where duplicated, indicating copying
log["Log: Oxygen Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Oxygen Duplicate"]]

# Nitrate Duplicates:

log['Log: Nitrate Bottle 1'] = log['Log: Nitrate Bottle 1'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Nitrate Bottle 1')
# Identify where the indices where duplicated, indicating copying
log["Log: Nitrate Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Nitrate Duplicate"]]

# Chlorophyll Brown Bottle Duplicates:

log['Log: Chlorophyll Brown Bottle #'] = log['Log: Chlorophyll Brown Bottle #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Chlorophyll Brown Bottle #')
# Identify where the indices where duplicated, indicating copying
log["Log: Chlorophyll Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Chlorophyll Duplicate"]]

# Chlorophyll Filter Sample Duplicates:

log['Log: Chlorophyll Filter Sample #'] = log['Log: Chlorophyll Filter Sample #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Chlorophyll Filter Sample #')
# Identify where the indices where duplicated, indicating copying
log["Log: Chlorophyll Filter Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Chlorophyll Filter Duplicate"]]

log.shape

# **===================================================================================================================** 
# ### Salinity & Oxygen Data
# Next, add in the salinity and oxygen data to the log, keying off of the sample bottles

sal = pd.read_csv(salts_and_o2_path+'SAL_Summary.csv')
sal.drop(columns={'Unnamed: 0'}, inplace=True)
# Rename the columns to name source
for colname in list(sal.columns.values):
    sal.rename(columns={colname: 'Sal: ' + colname}, inplace=True)
sal['Sal: Salts Bottle #'] = sal['Sal: Case'] + sal['Sal: Sample ID'].apply(lambda x: str(x))
sal.drop(columns=['Sal: Sample ID','Sal: Case'], inplace=True)
# Check out the salinity data
sal.head()

# Load the oxygen data
oxy = pd.read_csv(salts_and_o2_path+'OXY_Summary.csv')
oxy.drop(columns={'Unnamed: 0'}, inplace=True)
# Rename the columns to name the source
for colname in list(oxy.columns.values):
    oxy.rename(columns={colname: 'Oxy: ' + colname}, inplace=True)
oxy['Oxy: Oxygen Bottle #'] = oxy['Oxy: Case'] + oxy['Oxy: Sample ID'].apply(lambda x: str(x))
oxy.drop(columns=['Oxy: Sample ID','Oxy: Case'], inplace=True)
oxy.head()

# Merge the salinity values
log = log.merge(sal, how='left', left_on=['Log: Station-Cast #', 'Log: Salts Bottle #'], right_on=['Sal: Station','Sal: Salts Bottle #'])
# Merge the oxygen values
log = log.merge(oxy, how='left', left_on=['Log: Station-Cast #', 'Log: Oxygen Bottle #'], right_on=['Oxy: Station','Oxy: Oxygen Bottle #'])

# This block checks the merge
log[log["Log: Oxygen Bottle #"].notna()][["Log: Salts Bottle #","Sal: Salts Bottle #","Log: Oxygen Bottle #","Oxy: Oxygen Bottle #"]]

# Drop unnecessary columns
drop_columns = ['Sal: Cruise','Sal: Station','Sal: Niskin','Sal: Salts Bottle #','Oxy: Cruise','Oxy: Station','Oxy: Oxygen Bottle #','Oxy: Units']
log.drop(columns=drop_columns, inplace=True)

dupes = log.duplicated()
log[dupes]

# **If there are no oxygen or salinity data:**

# If there are now salinity or oxygen data
log["Oxy: Oxygen [mL/L]"] = log["Log: Oxygen Bottle #"]
log["Sal: Salinity [psu]"] = log["Log: Salts Bottle #"]

# **===================================================================================================================**
# ### Chlorophyll Data

chl = pd.read_excel(chl_path, sheet_name='Chl')
# Collect the subset of relevant columns 
chl = chl[["Station-Cast #","Niskin #","Filter \nSample #","Chl (ug/l)","Phaeo (ug/l)","Comments"]]
# Rename the columns to name the source
for colname in list(chl.columns.values):
    chl.rename(columns = {colname: "Chl: " + colname}, inplace=True)
# Remove the spaces from filter sample number
chl["Chl: Filter Sample #"] = chl["Chl: Filter \nSample #"].apply(lambda x: x.replace(" ",""))
chl

# Clean up the log filter sample names
log['Log: Chlorophyll Filter Sample #'] = log["Log: Chlorophyll Filter Sample #"].apply(lambda x: x.replace('-','/') if not pd.isnull(x) else x)
log[log["Log: Chlorophyll Filter Sample #"].notna()]["Log: Chlorophyll Filter Sample #"]

# Now merge the chlorophyll with the sampling log
log = log.merge(chl, how="left", left_on=["Log: Chlorophyll Filter Sample #"], right_on=["Chl: Filter \nSample #"])

for i in range(len(log)):
    a = log["Log: Chlorophyll Filter Sample #"].iloc[i]
    b = log["Chl: Filter Sample #"].iloc[i]
    if a != b:
        if pd.isnull(a) and pd.isnull(b):
            pass
        else:
            print(str(a) + ": " + str(b))

# Drop unnecessary columns
log.drop(columns=["Chl: Station-Cast #","Chl: Niskin #", "Chl: Filter Sample #"], inplace=True)
log

# **If there is no chlorophyll data yet, need to fill it in with the sample IDs**

# If there is no chlorophyll data yet, need to fill it in 
log["Chl: Chl (ug/l)"] = log["Log: Chlorophyll Filter Sample #"].apply(lambda x: x.replace('-','/') if type(x) == str else x)
log["Chl: Phaeo (ug/l)"] = log["Chl: Chl (ug/l)"]
log["Chl: Comments"] = None

# **===================================================================================================================**
# ### Nutrients

nuts = pd.read_excel(nutrients_path, sheet_name='Summary')

# Rename the columns to name the source
for colname in list(nuts.columns.values):
    nuts.rename(columns={colname: "Nuts: " + colname.replace('Avg: ','')}, inplace=True)
nuts

# Now merge the nutrients data into the sampling log data
log = log.merge(nuts, how="left", left_on=["Log: Nitrate Bottle 1"], right_on=["Nuts: Sample ID"])

# Now check that the merge worked
log[log['Log: Nitrate Bottle 1'].apply(type) != float][["Log: Nitrate Bottle 1","Nuts: Sample ID"]]

# Eliminate the extraneous columns
log.drop(columns=["Nuts: Sample ID"], inplace=True)
log.head()

# **If there is no nutrients data yet, need to fill it in with sample IDs**

nuts_columns = ['Nuts: Nitrate [µmol/L]', 'Nuts: Phosphate [µmol/L]', 'Nuts: Silicate [µmol/L]', 'Nuts: Nitrate+Nitrite [µmol/L]', 'Nuts: Ammonium [µmol/L]','Nuts: Nitrite [µmol/L]']
for col in nuts_columns:
    log[col] = log["Log: Nitrate Bottle 1"]

# **===================================================================================================================**
# ### DIC 

dic = pd.read_excel(dic_path, header=1)
dic.head()

dic['CRUISE_ID']

# Get only the DIC values for a leg of a cruise
# cruise_id = 'AR04-A'
dic = dic[dic['CRUISE_ID'] == cruise_id]
dic

# Select a subset of the columns needed
dic = dic[["CAST_NO", "NISKIN_NO", "DIC_UMOL_KG", "DIC_FLAG_W", "TA_UMOL_KG", "TA_FLAG_W", "PH_TOT_MEA", "TMP_PH_DEG_C", "PH_FLAG_W"]]
# Rename the columns to name the source
for colname in list(dic.columns.values):
    dic.rename(columns={colname: "DIC: " + colname}, inplace=True)
dic.head()

# Merge the carbon data into the sampling log
log = log.merge(dic, how="left", left_on=["Log: Station-Cast #","Log: Niskin #"], right_on=["DIC: CAST_NO", "DIC: NISKIN_NO"])
# log= log.merge(dic, how="left", left_on=["Log: DIC/TA Bottle #"], right_on=["DIC: SAMPLE_ID"])
# Check that the merge worked
# log[log['DIC: CAST_NO'].apply(np.isnan) == False][['Log: Station-Cast #', 'Log: Niskin #', 'DIC: CAST_NO', 'DIC: NISKIN_NO']]

# Drop unnecessary columns
log.drop(columns=["DIC: CAST_NO","DIC: NISKIN_NO"], inplace=True)
# log.drop(columns=["DIC: SAMPLE_ID"], inplace=True)
log.head()

# **If there is no carbon system data yet, need to fill it in with sample IDs**

dic_columns = ["DIC: CAST_NO", "DIC: NISKIN_NO", "DIC: DIC_UMOL_KG", "DIC: DIC_FLAG_W", "DIC: TA_UMOL_KG", "DIC: TA_FLAG_W", "DIC: PH_TOT_MEA", "DIC: TMP_PH_DEG_C", "DIC: PH_FLAG_W"]
for col in dic_columns:
    if 'ph' in col.lower():
        log[col] = log["Log: Ph Bottle #"]
    else:
        log[col] = log["Log: DIC/TA Bottle #"]

# **===================================================================================================================**
# ### CTD Data

# Load the CTD Data
ctd = pd.read_csv(sample_dir + 'CTD_Summary.csv')
ctd.drop(columns='Unnamed: 0', inplace=True)
# Replace the CTD w/CTD: for easier post-processing
for colname in list(ctd.columns.values):
    ctd.rename(columns={colname: 'CTD: ' + colname.replace('CTD ',"")}, inplace=True)
ctd.head()

ctd["CTD: Cast"] = ctd["CTD: Cast"].apply(lambda x: x.lstrip('0'))

# With the CTD data and the Log Data, I want to merge _only_ on the log columns, because those are the measurements we made. This is because the CTD data is more accurate in the measurements.

df = log.merge(ctd, how="outer", left_on=["Log: Station-Cast #", "Log: Niskin #"], right_on=["CTD: Cast","CTD: Bottle Position"])
df.drop_duplicates(inplace=True)
df.info()

[x for x in df.columns.values if 'duplicate' in x.lower()]

# Select a subset of the available columns which will be kept
columns = ['Log: Cruise ID',
           'Log: Station-Cast #',
           'Log: Target Station',
           'CTD: Start Latitude [degrees]', 
           'CTD: Start Longitude [degrees]',
           'CTD: Start Time [UTC]',
           'Log: Bottom Depth [m]',
           'CTD: Filename',
           'CTD: Bottle Position',
           'CTD: Date Time', 
           'CTD: Pressure, Digiquartz [db]', 
           'CTD: Depth [salt water, m]',
           'CTD: Latitude [deg]', 
           'CTD: Longitude [deg]', 
           'CTD: Temperature [ITS-90, deg C]',
           'CTD: Temperature, 2 [ITS-90, deg C]', 
           'CTD: Conductivity [S/m]', 
           'CTD: Conductivity, 2 [S/m]', 
           'CTD: Salinity, Practical [PSU]', 
           'CTD: Salinity, Practical, 2 [PSU]', 
           'CTD: Oxygen, SBE 43 [ml/l]', 
           'CTD: Oxygen Saturation, Garcia & Gordon [ml/l]', 
           'CTD: Beam Attenuation, WET Labs C-Star [1/m]',
           'CTD: Beam Transmission, WET Labs C-Star [%]', 
           'Oxy: Oxygen [mL/L]', 
           'Log: Oxygen Duplicate',
           'Chl: Chl (ug/l)', 
           'Chl: Phaeo (ug/l)', 
           'Log: Chlorophyll Duplicate', 
           'Log: Chlorophyll Filter Duplicate',
           'Nuts: Nitrate', # [µmol/L]',
           'Nuts: Phosphate',# [µmol/L]',
           'Nuts: Silicate', # [µmol/L]', 
           'Nuts: Nitrate+Nitirte',# [µmol/L]',
           'Nuts: Ammonium',# [µmol/L]', 
           'Nuts: Nitrite',# [µmol/L]', 
           'Log: Nitrate Duplicate',
           'Sal: Salinity [psu]',
           'Log: Salts Duplicate',
           'DIC: TA_UMOL_KG', 
           'DIC: TA_FLAG_W', 
           'DIC: DIC_UMOL_KG', 
           'DIC: DIC_FLAG_W',
           'DIC: PH_TOT_MEA', 
           'DIC: TMP_PH_DEG_C', 
           'DIC: PH_FLAG_W',
           'Log: Comments', 
           'Chl: Comments']

summary = df[columns]
summary = summary.sort_values(by=["Log: Station-Cast #","CTD: Bottle Position"])

summary['Log: Bottom Depth [m]'] = summary['Log: Bottom Depth [m]'].apply(lambda x: x.replace('m','').strip() if type(x) == str else x)

# Now, I need to fill in metadata columns based on other columns to make as complete a dataset as is possible
summary['Log: Station-Cast #'] = summary['Log: Station-Cast #'].fillna(value=df['CTD: Cast'])
summary['CTD: Start Latitude [degrees]'] = summary['CTD: Start Latitude [degrees]'].fillna(value=df['Log: Start Latitude'])
summary['CTD: Start Longitude [degrees]'] = summary['CTD: Start Longitude [degrees]'].fillna(value=df['Log: Start Longitude'])
summary['CTD: Start Time [UTC]'] = summary['CTD: Start Time [UTC]'].fillna(value=df['Log: Start Date'])
summary['CTD: Bottle Position'] = summary['CTD: Bottle Position'].fillna(value=df['Log: Niskin #'])

summary = summary.sort_values(by=['Log: Station-Cast #','CTD: Bottle Position'])
summary

# Create a straight-up mapping instead of this weird fuzzy-wuzzy matching
name_map = {
    'Cruise': 'Log: Cruise ID',
    'Station': 'Log: Station-Cast #',
    'Target Asset': 'Log: Target Station',
    'Start Latitude [degrees]': 'CTD: Start Latitude [degrees]',
    'Start Longitude [degrees]': 'CTD: Start Longitude [degrees]',
    'Start Time [UTC]': 'CTD: Start Time [UTC]',
    'Cast': 'Log: Station-Cast #',
    'Cast Flag': None,
    'Bottom Depth at Start Position [m]': 'Log: Bottom Depth [m]',
    'File': 'CTD: Filename',
    'File Flag': None,
    'Niskin/Bottle Position': 'CTD: Bottle Position',
    'Niskin Flag': None,
    'Bottle Closure Time [UTC]': 'CTD: Date Time',
    'Pressure [db]': 'CTD: Pressure, Digiquartz [db]',
    'Pressure Flag': None,
    'Depth [m]': 'CTD: Depth [salt water, m]',
    'Latitude [deg]': 'CTD: Latitude [deg]',
    'Longitude [deg]': 'CTD: Longitude [deg]',
    'Temperature 1 [deg C]': 'CTD: Temperature [ITS-90, deg C]',
    'Temperature 1 Flag': None,
    'Temperature 2 [deg C]': 'CTD: Temperature, 2 [ITS-90, deg C]',
    'Temperature 2 Flag': None,
    'Conductivity 1 [S/m]': 'CTD: Conductivity [S/m]',
    'Conductivity 1 Flag': None,
    'Conductivity 2 [S/m]': 'CTD: Conductivity, 2 [S/m]',
    'Conductivity 2 Flag': None,
    'Salinity 1, uncorrected [psu]': 'CTD: Salinity, Practical [PSU]',
    'Salinity 2, uncorrected [psu]': 'CTD: Salinity, Practical, 2 [PSU]',
    'Oxygen, uncorrected [mL/L]': 'CTD: Oxygen, SBE 43 [ml/l]',
    'Oxygen Flag': None,
    'Oxygen Saturation [mL/L]': 'CTD: Oxygen Saturation, Garcia & Gordon [ml/l]',
    'Fluorescence [mg/m^3]': None,
    'Fluorescence Flag': None,
    'Beam Attenuation [1/m]': 'CTD: Beam Attenuation, WET Labs C-Star [1/m]',
    'Beam Transmission [%]': 'CTD: Beam Transmission, WET Labs C-Star [%]',
    'Transmissometer Flag': None,
    'pH': None,
    'pH Flag': None,
    'Discrete Oxygen [mL/L]': 'Oxy: Oxygen [mL/L]',
    'Discrete Oxygen Flag': None,
    'Discrete Oxygen Duplicate Flag': 'Log: Oxygen Duplicate',
    'Discrete Chlorophyll [ug/L]': 'Chl: Chl (ug/l)',
    'Discrete Phaeopigment [ug/L]': 'Chl: Phaeo (ug/l)',
    'Discrete Fo/Fa Ratio': None,
    'Discrete Fluorescence Flag': None,
    'Discrete Fluorescence Duplicate Flag': 'Log: Chlorophyll Duplicate',
    'Discrete Phosphate [uM]': 'Nuts: Phosphate', #' [µmol/L]',
    'Discrete Silicate [uM]': 'Nuts: Silicate',# [µmol/L]',
    'Discrete Nitrate [uM]': 'Nuts: Nitrate',# [µmol/L]',
    'Discrete Nitrite [uM]': 'Nuts: Nitrite',# [µmol/L]',
    'Discrete Ammonium [uM]': 'Nuts: Ammonium',# [µmol/L]',
    'Discrete Nutrients Flag': None,
    'Discrete Nutrients Duplicate Flag': 'Log: Nitrate Duplicate',
    'Discrete Salinity [psu]': 'Sal: Salinity [psu]',
    'Discrete Salinity Flag': None,
    'Discrete Salinity Duplicate Flag': 'Log: Salts Duplicate',
    'Discrete Alkalinity [µmol/kg]': 'DIC: TA_UMOL_KG',
    'Discrete Alkalinity Flag': 'DIC: TA_FLAG_W',
    'Discrete DIC [µmol/kg]': 'DIC: DIC_UMOL_KG',
    'Discrete DIC Flag': 'DIC: DIC_FLAG_W',
    'Discrete pCO2 [µatm]': None, 
    'Discrete pCO2 Analysis Temp [C]': None,
    'Discrete pCO2 Flag': None,
    'Discrete pH [Total scale]': 'DIC: PH_TOT_MEA',
    'Discrete pH Analysis Temp [C]': 'DIC: TMP_PH_DEG_C',
    'Discrete pH Flag': 'DIC: PH_FLAG_W',
    'Calculated Alkalinity [µmol/kg]': None,
    'Calculated DIC [µmol/kg]': None,
    'Calculated pCO2 [µatm]': None,
    'Calculated pH': None,
    'Calculated CO2aq [µmol/kg]': None,
    'Calculated bicarb [µmol/kg]': None,
    'Calculated CO3 [µmol/kg]': None,
    'Calculated Omega-C': None,
    'Calculated Omega-A': None,
    'Comments': 'Log: Comments',
    'Chl Comments': 'Chl: Comments'
}

final = pd.DataFrame()
for key in name_map:
    column = name_map.get(key)
    if column is not None:
        final[key] = summary[column]
    else:
        final[key] = None

final.sort_values(by=['Cruise','Station','Niskin/Bottle Position'], inplace=True)

final

final.drop_duplicates(inplace=True)
final

cruise_id

# cruise_id = 'AR-31A'
final['Cruise'] = final['Cruise'].fillna(value=cruise_id)

cruise_name = cruise.replace('/','').split('_')[0]
current_date = pd.to_datetime(pd.datetime.now()).tz_localize(tz='US/Eastern').tz_convert(tz='UTC')
version = '1-2'

cruise_id, cruise_name

filename = '_'.join([cruise_name,cruise_id,'Discrete','Summary',current_date.strftime('%Y-%m-%d'),'ver',version,'.csv'])
filename

final.fillna(value=-9999999,inplace=True)

final.to_csv(basepath+array+cruise+filename, index=False)



# ## Update summary sheets
#

basepath = '/home/andrew/Documents/OOI-CGSN/QAQC_Sandbox/Ship_data/'

os.listdir(basepath+'Pioneer')

basepath = '/home/andrew/Documents/OOI-CGSN/QAQC_Sandbox/Ship_data/'
array = 'Pioneer/'
cruise =   'Pioneer-08_AR-18_2017-05-30/'

sorted(os.listdir(basepath+array+cruise))

water = 'Water Sampling/'
ctd = 'ctd/'
leg = 'Leg 1 (ar08a)/'

sorted(os.listdir(basepath + array + cruise + water))

sample_dir = basepath + array + cruise + ctd
water_dir = basepath + array + cruise + water
log_path = water_dir + 'Pioneer-08_AR-18A_CTD_Sampling_Log.xlsx'
chl_path = water_dir + ''
nutrients_path = water_dir + 'Pioneer-08_AR-18_DIC_Sample_Data_2019-11-06_ver_1-00.xlsx'
salts_and_o2_path = water_dir + 'Pioneer-08_AR-18A_Oxygen_Salinity_Sample_Data/'


data_path = basepath + array + cruise + 'Pioneer-08_AR-18_Discrete_Summary_2019-11-04_ver_1-02_.csv'

data = pd.read_csv(data_path)
data

data.columns

# Load the relevant data
dic_path = basepath + array + cruise + water + 'Pioneer-08_AR-18_DIC_Sample_Data_2019-11-06_ver_1-00.xlsx'
dic = pd.read_excel(dic_path)
dic.head()


def reformat_cruise_id(x):
    if '-A' in x:
        x = 'AR-18A'
    elif '-B' in x:
        x = 'AR-18B'
    elif '-C' in x:
        x = 'AR-18C'
    else:
        x
    return x


dic["CRUISE_ID"] = dic["CRUISE_ID"].apply(lambda x: reformat_cruise_id(x) )

df = data.merge(dic, how='left', left_on=['Cruise', 'Station', 'Niskin/Bottle Position'], right_on = ["CRUISE_ID", "CAST_NO", "NISKIN_NO"])

df["Discrete DIC [µmol/kg]"] = df["DIC_UMOL_KG"]
df["Discrete DIC Flag"] = df['DIC_FLAG_W']
df["Discrete Alkalinity [µmol/kg]"] = df["TA_UMOL_KG"]
df["Discrete Alkalinity Flag"] = df['TA_FLAG_W']
df["Discrete pH [Total scale]"] = df['PH_TOT_MEA']
df["Discrete pH Analysis Temp [C]"] = df["TMP_PH_DEG_C"]
df["Discrete pH Flag"] = df["PH_FLAG_W"]

df[df["Discrete pH [Total scale]"].notna()]["Discrete pH [Total scale]"]

df.drop(columns=[x for x in dic.columns], inplace=True)

df.columns

df.fillna(value=-9999999,inplace=True)


def fill_flags(x):
    if x == -9999999 or x == '-9999999':
        return x
    elif str(x).lower() == 'false':
        return x
    elif str(x).lower() == 'true':
        return x
    else:
        return str(x).zfill(16)


for column in df.columns:
    if 'flag' in column.lower():
        df[column] = df[column].apply(lambda x: fill_flags(x))

df

df.to_csv(basepath+array+cruise+'Pioneer-08_AR-18_Discrete_Summary_2019-11-07_ver_1-03_.csv', index=False)

data


