# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.6.0
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# # Bottle Processing
# Author: Andrew Reed
#
# ### Motivation:
# Independent verification of the suite of physical and chemical observations provided by OOI are critical for the observations to be of use for scientifically valid investigations. Consequently, CTD casts and Niskin water samples are made during deployment and recovery of OOI platforms, vehicles, and instrumentation. The water samples are subsequently analyzed by independent labs for  comparison with the OOI telemetered and recovered data.
#
# However, currently the water sample data routinely collected and analyzed as part of the OOI program are not available in a standardized format which maps the different chemical analyses to the physical measurements taken at bottle closure. Our aim is to make these physical and chemical analyses of collected water samples available to the end-user in a standardized format for easy comprehension and use, while maintaining the source data files. 
#
# ### Approach:
# Generating a summary of the water sample analyses involves preprocessing and concatenating multiple data sources, and accurately matching samples with each other. To do this, I first preprocess the ctd casts to generate bottle (.btl) files using the SeaBird vendor software following the SOP available on Alfresco. 
#
# Next, the bottle files are parsed using python code and the data renamed following SeaBird's naming guide. This creates a series of individual cast summary (.sum) files. These files are then loaded into pandas dataframes, appended to each other, and exported as a csv file containing all of the bottle data in a single data file.
#
# ### Data Sources/Software:
#
# * **sbe_name_map**: This is a spreadsheet which maps the short names generated by the SeaBird SBE DataProcessing Software to the associated full names. The name mapping originates from SeaBird's SBE DataProcessing support documentation.
#
# * **Alfresco**: The Alfresco CMS for OOI at alfresco.oceanobservatories.org is the source of the ctd hex, xmlcon, and psa files necessary for generating the bottle files needed to create the sample summary sheet.
#
# * **SBEDataProcessing-Win32**: SeaBird vendor software for processing the raw ctd files and generating the .btl files.
#

import os, sys, re
import pandas as pd
import numpy as np
import datetime
import pytz
import warnings

sbe_name_map = pd.read_excel('/media/andrew/OS/Users/areed/Documents/OOI-CGSN/QAQC_Sandbox/Reference_Files/seabird_ctd_name_map.xlsx')

sbe_name_map.head()

column_order = pd.read_excel("/media/andrew/Files/Water_Sampling/column_order.xlsx")
column_order = column_order.columns

Summary = pd.DataFrame(columns=column_order)
Summary

# ---
# Set the directory paths to where the relevant information is stored:

basepath = "/media/andrew/Files/Water_Sampling/"
array = 'Global_Station_Papa_Array/'

# List the cruises:

for cruise in sorted(os.listdir(basepath + array)):
    print(cruise)

cruise = "Station_Papa-07_SKQ201920S_2019-09-18" + "/" + "Ship_Data/"

# List the legs and water sampling directories:

for leg in sorted(os.listdir(basepath + array + cruise)):
    print(leg)

# water = "Water Sampling/"
ctd = "ctd/raw/SKQ201920S/"
# ctd = "at26-30/" + "ctd/"

# List the CTD bottle data:

for file in sorted(os.listdir(basepath + array + cruise + ctd)):
    print(file)

# #### Load Bottle Data

BTL_DIR = basepath + array + cruise + ctd

os.listdir(BTL_DIR)

# ---
# ## BTL Data
# First, load in the bottle data from the CTD casts. This data should already have been processed using the SeaBird Processing software to produce the relevant .btl files.

from bottle_utils import Cast

# Iterate through the directory where the bottle files are stored, parsing them into a single dataframe:

# +
Bottles = pd.DataFrame()

for file in os.listdir(BTL_DIR):
    if file.endswith(".btl"):
        # Get the cast number from the file name
        cast_no = file[file.find(".")-3:file.find(".")]
        try:
            cast_no = int(cast_no)
        except ValueError:
            cast_no = cast_no.lstrip("0")
        
        # Initialize the CTD Cast object
        cast = Cast(cast_no)
        
        # Parse the cast data
        cast.parse_cast(BTL_DIR+file)
        
        # Add in the cast number
        df = pd.DataFrame(cast.data)
        
        for key, item in cast.header.items():
            df.insert(1, key, item)

        df.insert(0, "Cast", cast.cast_number.zfill(3))
        
        # Save the results in a dataframe
        Bottles = Bottles.append(df, ignore_index=True)
        
Bottles.head()
# -

Bottles["Longitude"] = Bottles["Longitude"].apply(lambda x: float("-"+str(x)))

Bottles["Longitude"]

Bottles["Date Time"] = Bottles["Date Time"].apply(lambda x: pd.to_datetime(x).strftime("%Y-%m-%dT%H:%M:%S.000Z"))
Bottles["Start Time [UTC]"] = Bottles["Start Time [UTC]"].apply(lambda x: pd.to_datetime(x).strftime("%Y-%m-%dT%H:%M:%S.000Z"))
Bottles;

# +
# Rename the column title using the sbe_name_mapping 
for colname in list(Bottles.columns.values):
    try:
        fullname = list(sbe_name_map[sbe_name_map['Short Name'].apply(lambda x: str(x).lower() == colname.lower()) == True]['Full Name'])[0]
        Bottles.rename({colname:fullname},axis='columns',inplace=True)
    except:
        pass
    
Bottles.head()
# -
Bottles["Cast"].unique()

# Save the bottle data


date = datetime.datetime.now(tz=pytz.UTC).strftime("%Y-%m-%d")
filename = f"Station_Papa-07_SKQ201920S_CTD_Bottle_Data_{date}_Ver_1-00.xlsx"
filename

SAVE_PATH = basepath + array + cruise + "Water_Sampling"
os.listdir(SAVE_PATH)

SAVE_FILE = SAVE_PATH + "/" + filename
SAVE_FILE

Bottles.to_excel(SAVE_FILE, index=False)

# ---
# ## CTD Log 
#
# First, find the directory of where to find the CTD Log for the cruise:

for file in sorted(os.listdir(basepath + array + cruise + water)):
    if "CTD" in file:
        print(file)

# Next, set the paths to load the Log files for the cruise(s):

LOG_FILE_A = basepath + array + cruise + water + "Pioneer-04_AT-27A_CTD_Sampling_Log_2020-05-11_ver_1-01.xlsx"
LOG_FILE_B = basepath + array + cruise + water + "Pioneer-04_AT-27B_CTD_Sampling_Log_2020-05-11_ver_1-01.xlsx"

Log_A = pd.read_excel(LOG_FILE_A, sheet_name="Summary")
Log_A.head()

Log_B = pd.read_excel(LOG_FILE_B, sheet_name="Summary")
Log_B.head()

# If the descriptors are in the first row, drop it:

Log_A = Log_A.drop(labels=0)
Log_B = Log_B.drop(labels=0)

# Clean up the entries for the salts bottles:

# +
Log_A["Salts Bottle #"] = Log_A["Salts Bottle #"].apply(lambda x: x.replace("/",",") if type(x) == str else x)

Log["Oxygen Bottle #"] = Log["Oxygen Bottle #"].apply(lambda x: x.replace("/",",") if type(x) == str else x)
# -

# Clean up headers:

# +
# Check for any carriage returns and remove
for colname in Log.columns:
    Log.rename(columns={colname: colname.replace("\n"," ")}, inplace=True)
    
# Sometimes spaces are missing before the #-symbol
for colname in Log.columns:
    Log.rename(columns={colname: colname.replace("#"," #")}, inplace=True)

# Check for extra spaces and remove
for colname in Log.columns:
    Log.rename(columns={colname: colname.replace("  ", " ")}, inplace=True)

# Check the results
Log.columns


# -

# Clean up the **station-cast #** so it is uniform type

def clean_station_num(x):
    if pd.isnull(x):
        return x
    else:
        x = str(x).zfill(3)
        return x


Log["Station-Cast #"] = Log["Station-Cast #"].apply(lambda x: clean_station_num(x))
Log["Station-Cast #"].unique()


# Clean up **Start Date** and **Start Time** into a single column

def combine_datetime(x, y):
    if pd.isnull(y):
        y = "00:00:00"
    dt = x + " " + y
    dt = pd.to_datetime(dt)
    return dt


Log["Start DateTime"] = Log[["Start Date","Start Time"]].apply(lambda x: combine_datetime(x[0], x[1]), axis=1)


# ---
# ## Merge Log and Bottle Sampling

def clean_bottle_pos(x):
    if pd.isnull(x):
        return x
    else:
        x = int(x)
        return x


Log["Niskin #"] = Log["Niskin #"].apply(lambda x: clean_bottle_pos(x))
Bottles["Bottle Position"] = Bottles["Bottle Position"].apply(lambda x: clean_bottle_pos(x))

# ---
# ## Salinity & Oxygen Data
# Next, need to load the Salinity and Oxygen discrete data into dataframes. The data should be summarized in an excel sheet following the naming scheme ```<Cruise Name>_<OXY/SAL>_Summary.xlsx```.

# **Salinity** <br>
# Load the Salinity data

Salinity = pd.read_excel(SAL_FILE, sheet_name="AT30-01")
Salinity.head()

# **Oxygen** <br>
# Load the Oxygen data

Oxygen = pd.read_excel(OXY_FILE, sheet_name="AT30-01")
Oxygen.head()

# ---
# ## Nutrients Data
# Sometimes the nutrient data needs to be 

Nutrients = pd.read_excel(NUT_FILE, sheet_name="Summary", header=2)
Nutrients

# Get the Niskin and Cast Numbers based on the Sample-ID

Nuts_keys = Log[["Station-Cast #", "Niskin #", "Nitrate Bottle 1"]].dropna().set_index(keys="Nitrate Bottle 1")
Nutrients = Nutrients.set_index(keys="Sample ID")
Nutrients = Nutrients.merge(Nuts_keys, left_index=True, right_index=True).reset_index().drop(columns=["Sample ID"])
Nutrients

# ---
# ## Chlorophyll Data

Chlorophyll = pd.read_excel(CHL_FILE)

Chlorophyll.columns

# Get only the relevant Chlorophyll data
columns = ["Cast", "Niskin", "Chl ug_per_L", "Phaeo ug_per_L", "Comments"]
Chlorophyll = Chlorophyll[columns]
Chlorophyll.head()

# ## DIC

DIC = pd.read_excel(DIC_FILE, header=1)

DIC.columns

columns = ["CAST_NO", "NISKIN_NO", 'DIC_UMOL_KG', 'DIC_FLAG_W', 'TA_UMOL_KG', 'TA_FLAG_W', 'PH_TOT_MEA', 'TMP_PH_DEG_C', 'PH_FLAG_W']
DIC = DIC[columns]
DIC.head()

# ## Union of Casts and Niskins
#
# First, want to get the combination of all Casts and Niskin Bottle #s for the cruise. 

# +
# First, get the casts and niskin numbers
bot = Bottles[["Cast","Bottle Position"]]
bot.rename(columns={"Cast": "Station", "Bottle Position": "Niskin/Bottle Position"}, inplace=True)
log = Log[["Station-Cast #", "Niskin #"]]
log.rename(columns={"Station-Cast #": "Station", "Niskin #":"Niskin/Bottle Position"}, inplace=True)

# Concatentate the casts and bottles
casts = bot.append(log, ignore_index=True)

# Group the results by casts and get the unique Niskin bottle numbers
casts = casts.groupby(by="Station").apply(np.unique)
casts.name = "Niskin/Bottle Position"

# Expand the unique cast-niskin combinations
casts_niskins = pd.DataFrame(casts).explode(column="Niskin/Bottle Position").reset_index()

# Drop NaNs from the Niskin/Bottle Position
casts_niskins.dropna(subset=["Niskin/Bottle Position"], inplace=True)
casts_niskins
# -

# ## Summary Spreadsheet

# ### 1: Initialize the Summary Spreadsheet

Summary = pd.DataFrame(columns=column_order)
Summary.head()

# ### 2: Add in the Cast Data

Summary = Summary.append(casts_niskins, ignore_index=True)
Summary.head()

# ### 3: Add Log Metadata
# Next, need to add in the metaata from the CTD Log. This includes the following columns: <br>
#
#     | Target Station | Start Latitude | Start Longitude | Start DateTime | Bottom Depth [m] |
#     | -------------- | -------------- | --------------- | -------------- | ---------------- |
#     | (str)          | (str)          | (str)           | (DateTime)     | (int)            |
#

metadata_columns = ["Station-Cast #","Target Station", "Start Latitude", "Start Longitude", "Start DateTime", "Bottom Depth [m]"]

metadata = Log[metadata_columns]
metadata = metadata.groupby(by="Station-Cast #").agg(np.unique)
metadata = metadata.reset_index()
metadata

Summary = Summary.merge(metadata, how="left", left_on="Station", right_on="Station-Cast #")

Summary["Target Asset"] = Summary["Target Asset"].combine_first(Summary["Target Station"])
Summary["Start Latitude [degrees]"] = Summary["Start Latitude [degrees]"].combine_first(Summary["Start Latitude"])
Summary["Start Longitude [degrees]"] = Summary["Start Longitude [degrees]"].combine_first(Summary["Start Longitude"])
Summary["Start Time [UTC]"] = Summary["Start Time [UTC]"].combine_first(Summary["Start DateTime"])
Summary["Bottom Depth at Start Position [m]"] = Summary["Bottom Depth at Start Position [m]"].combine_first(Summary["Bottom Depth [m]"])

# Drop the extra columns from the merged data

Summary.drop(columns=metadata_columns, inplace=True)
Summary.head()

# ### 4: Add Oxygen and Salinity Data

Oxygen["Station ID"] = Oxygen["Station ID"].apply(lambda x: str(x).zfill(3))
Oxygen.head()

# +
Summary_keys = ["Station","Niskin/Bottle Position"]
Oxygen_keys = ["Station ID", "Niskin ID"]

Summary = Summary.merge(Oxygen, how="left", left_on=Summary_keys, right_on=Oxygen_keys)
# -

Summary["Discrete Oxygen [mL/L]"] = Summary["Oxygen [mL/L]"]

# Drop the oxygen columns

Summary.drop(columns=Oxygen.columns, inplace=True)

# Salinity

Salinity["Station ID"] = Salinity["Station ID"].apply(lambda x: str(x).zfill(3))
Salinity.head()

# +
Summary_keys = ["Station","Niskin/Bottle Position"]
Salinity_keys = ["Station ID", "Niskin ID"]

Summary = Summary.merge(Salinity, how="left", left_on=Summary_keys, right_on=Salinity_keys)
# -

Summary["Discrete Salinity [psu]"] = Summary["Salinity [psu]"]

Summary.drop(columns=Salinity.columns, inplace=True)

# ## Add Chlorophyll Data

Chlorophyll["Cast"] = Chlorophyll["Cast"].apply(lambda x: str(x).zfill(3))
Chlorophyll.rename(columns={"Cast":"Cast_y"}, inplace=True)

Chlorophyll_keys = ["Cast_y", "Niskin"]

Summary = Summary.merge(Chlorophyll, how="left", left_on=Summary_keys, right_on=Chlorophyll_keys)
Summary

Summary["Discrete Chlorophyll [ug/L]"] = Summary["Chl ug_per_L"]
Summary["Discrete Phaeopigment [ug/L]"] = Summary["Phaeo ug_per_L"]
Summary["Chlorophyll Comments"] = Summary["Comments"]
Summary.drop(columns=Chlorophyll.columns, inplace=True)

# ## Merge the CTD Data
# Next, we want to merge the CTD onto the Station/Casts

# +
Summary_keys = ["Station", "Niskin/Bottle Position"]
Bottles_keys = ["BOT: Cast", "BOT: Bottle Position"]

# Merge the Summary and CTD Bottle Data
Summary = Summary.merge(Bottles, how="left", left_on=Summary_keys, right_on=Bottles_keys)

# Drop the Bottle Cast and Bottle Position Headers
Summary.drop(columns=["BOT: Bottle Position"], inplace=True)
Summary.head()
# -

# ## Merge the LOG Data

# +
Summary_keys = ["Station", "Niskin/Bottle Position"]
Log_keys = ["LOG: Station-Cast #", "LOG: Niskin #"]

Summary.merge(Log, how="left", left_on=Summary_keys, right_on=Log_keys)
# -



Log.columns

# ### Duplicate Samples
# Next, want to identify where there are duplicate samples for salts, oxygen, chlorophyll, and nutrients. The process is to:
# 1. Map duplicate samples into a list
# 2. Split the list into rows so each row has an unique entry from the list, copying all other column values
# 3. Replace the values
# 4. Create a new boolean column indicating the columns copied for each water property sample 

# #### 1. Split

fuck me

# +
# First, convert multiple entries into a list
Log["Log: Oxygen Bottle #"] = Log["Log: Oxygen Bottle #"].apply(lambda x: x.replace(" ","").split(",") if type(x) == str else x)

# Next, split the multiple samples into their own row
Log = Log.explode("Log: Oxygen Bottle #")
# -

# #### 2. Apply
# Now, take the oxygen data and merge it into the Water Sampling Summary Sheet based on the **Cast, Niskin, Bottle Sample ID**

for colname in Oxygen:
    Oxygen.rename(columns={colname: "Oxy: " + colname}, inplace=True)
Oxygen

Oxygen.columns

pd.merge(Log, Oxygen, how="left", left_on=["Log: Station-Cast #", "Log: Niskin #", "Log: Oxygen Bottle #"], right_on=["Oxy: Station ID", "Oxy: Niskin ID", "Oxy: Sample ID"])

# Merge the Data
Log.merge(Oxygen, how="left", left_on=[["Log: Station-Cast #", "Log: Niskin #", "Log: Oxygen Bottle #"]], right_on=[["Oxy: Station ID", "Oxy: Niskin ID", "Oxy: Sample ID"]])

Log[["Log: Station-Cast #", "Log: Niskin #", "Log: Oxygen Bottle #"]]

Oxygen[['Oxy: Station ID', 'Oxy: Niskin ID', 'Oxy: Sample ID']]







# **===================================================================================================================**
# ### Load the Sampling Log 

# Load the CTD Log csv
log = pd.read_excel(log_path, sheet_name='Summary')
# Add "Log" to the column name
for colname in list(log.columns.values):
    log.rename(columns={colname:'Log: '+colname}, inplace=True)
log.rename(columns={'Log:  Oxygen Bottle #':'Log: Oxygen Bottle #'}, inplace=True)

log.head()

log["Log: Start Time"] = log["Log: Start Time"].apply(str)

log["Log: Start Time"] = log["Log: Start Time"].apply(lambda x: pd.to_datetime(x,format='%H%M') if type(x) == str else x)
# Next, need to reformat the Date and Time columns to be pandas datetime objects and merge into a single column
log['Log: Start Date'] = log['Log: Start Date'].apply(lambda x: pd.to_datetime(x).strftime('%Y-%m-%d') if not pd.isnull(x) else '')
log['Log: Start Time'] = log['Log: Start Time'].apply(lambda x: x.strftime('%H:%M:%SZ') if not pd.isnull(x) else '00:00:00Z')
# Now combine the two columns into a single column
log['Log: Start Date'] = log['Log: Start Date'] + 'T' + log['Log: Start Time']
# Check if any start with "T" indicating no date - will replace with ''
log['Log: Start Date'] = log['Log: Start Date'].apply(lambda x: '' if x.startswith('T') else x)
# Drop the "Start Time" column because it is not needed
log.drop(columns='Log: Start Time', inplace=True)

# Now get the cruise id
cruise_id = log[log["Log: Cruise ID"].notna()]["Log: Cruise ID"].unique()[0]
cruise_id

# **If there is no sampling log (but ctd casts)**:

columns = ['Log: Cruise ID', 'Log: Station-Cast #', 'Log: Target Asset', 'Log: Start Latitude', 
           'Log: Start Longitude', 'Log: Start Date', 'Log: Start Time', 'Log: Bottom Depth [m]', 
           'Log: Date', 'Log: Niskin #', 'Log: Time', 'Log: Trip Depth', 'Log: Oxygen Bottle #',
           'Log: Ph Bottle #', 'Log: DIC/TA Bottle #', 'Log: Salts Bottle #', 'Log: Nitrate Bottle 1',
           'Log: Chlorophyll Brown Bottle #', 'Log: Chlorophyll Filter Sample #', 
           'Log: Chlorophyll Brown Bottle Volume', 'Log: Chlorophyll LN Tube', 'Log: Comments']

log = pd.DataFrame(columns=columns)

# Salts Duplicates:

# Next, we need to turn any salts duplicates into a list, map the list into rows with unique entries, then identify
# those rows which were copied
log['Log: Salts Bottle #'] = log['Log: Salts Bottle #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Salts Bottle #')
# Identify where the indices where duplicated, indicating copying
log["Log: Salts Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Salts Duplicate"]]

# Oxygen Duplicates:

log['Log: Oxygen Bottle #'] = log['Log: Oxygen Bottle #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Oxygen Bottle #')
# Identify where the indices where duplicated, indicating copying
log["Log: Oxygen Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Oxygen Duplicate"]]

# Nitrate Duplicates:

log['Log: Nitrate Bottle 1'] = log['Log: Nitrate Bottle 1'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Nitrate Bottle 1')
# Identify where the indices where duplicated, indicating copying
log["Log: Nitrate Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Nitrate Duplicate"]]

# Chlorophyll Brown Bottle Duplicates:

log['Log: Chlorophyll Brown Bottle #'] = log['Log: Chlorophyll Brown Bottle #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Chlorophyll Brown Bottle #')
# Identify where the indices where duplicated, indicating copying
log["Log: Chlorophyll Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Chlorophyll Duplicate"]]

# Chlorophyll Filter Sample Duplicates:

log['Log: Chlorophyll Filter Sample #'] = log['Log: Chlorophyll Filter Sample #'].apply(lambda x: x.replace(' ','').split(',') if type(x) == str else x)
# Put each sample into its own row
log = log.explode(column='Log: Chlorophyll Filter Sample #')
# Identify where the indices where duplicated, indicating copying
log["Log: Chlorophyll Filter Duplicate"] = log.index.duplicated(keep=False)
# Drop rows which are duplicates (but not necessarily duplicate indices)
log = log.drop_duplicates()
# Reset the index to wipe the memory
log = log.reset_index(drop=True)
# Check for rows which were duplicate samples
log[log["Log: Chlorophyll Filter Duplicate"]]

log.shape

# **===================================================================================================================** 
# ### Salinity & Oxygen Data
# Next, add in the salinity and oxygen data to the log, keying off of the sample bottles

sal = pd.read_csv(salts_and_o2_path+'SAL_Summary.csv')
sal.drop(columns={'Unnamed: 0'}, inplace=True)
# Rename the columns to name source
for colname in list(sal.columns.values):
    sal.rename(columns={colname: 'Sal: ' + colname}, inplace=True)
sal['Sal: Salts Bottle #'] = sal['Sal: Case'] + sal['Sal: Sample ID'].apply(lambda x: str(x))
sal.drop(columns=['Sal: Sample ID','Sal: Case'], inplace=True)
# Check out the salinity data
sal.head()

# Load the oxygen data
oxy = pd.read_csv(salts_and_o2_path+'OXY_Summary.csv')
oxy.drop(columns={'Unnamed: 0'}, inplace=True)
# Rename the columns to name the source
for colname in list(oxy.columns.values):
    oxy.rename(columns={colname: 'Oxy: ' + colname}, inplace=True)
oxy['Oxy: Oxygen Bottle #'] = oxy['Oxy: Case'] + oxy['Oxy: Sample ID'].apply(lambda x: str(x))
oxy.drop(columns=['Oxy: Sample ID','Oxy: Case'], inplace=True)
oxy.head()

# Merge the salinity values
log = log.merge(sal, how='left', left_on=['Log: Station-Cast #', 'Log: Salts Bottle #'], right_on=['Sal: Station','Sal: Salts Bottle #'])
# Merge the oxygen values
log = log.merge(oxy, how='left', left_on=['Log: Station-Cast #', 'Log: Oxygen Bottle #'], right_on=['Oxy: Station','Oxy: Oxygen Bottle #'])

# This block checks the merge
log[log["Log: Oxygen Bottle #"].notna()][["Log: Salts Bottle #","Sal: Salts Bottle #","Log: Oxygen Bottle #","Oxy: Oxygen Bottle #"]]

# Drop unnecessary columns
drop_columns = ['Sal: Cruise','Sal: Station','Sal: Niskin','Sal: Salts Bottle #','Oxy: Cruise','Oxy: Station','Oxy: Oxygen Bottle #','Oxy: Units']
log.drop(columns=drop_columns, inplace=True)

dupes = log.duplicated()
log[dupes]

# **If there are no oxygen or salinity data:**

# If there are now salinity or oxygen data
log["Oxy: Oxygen [mL/L]"] = log["Log: Oxygen Bottle #"]
log["Sal: Salinity [psu]"] = log["Log: Salts Bottle #"]

# **===================================================================================================================**
# ### Chlorophyll Data

chl = pd.read_excel(chl_path, sheet_name='Chl')
# Collect the subset of relevant columns 
chl = chl[["Station-Cast #","Niskin #","Filter \nSample #","Chl (ug/l)","Phaeo (ug/l)","Comments"]]
# Rename the columns to name the source
for colname in list(chl.columns.values):
    chl.rename(columns = {colname: "Chl: " + colname}, inplace=True)
# Remove the spaces from filter sample number
chl["Chl: Filter Sample #"] = chl["Chl: Filter \nSample #"].apply(lambda x: x.replace(" ",""))
chl

# Clean up the log filter sample names
log['Log: Chlorophyll Filter Sample #'] = log["Log: Chlorophyll Filter Sample #"].apply(lambda x: x.replace('-','/') if not pd.isnull(x) else x)
log[log["Log: Chlorophyll Filter Sample #"].notna()]["Log: Chlorophyll Filter Sample #"]

# Now merge the chlorophyll with the sampling log
log = log.merge(chl, how="left", left_on=["Log: Chlorophyll Filter Sample #"], right_on=["Chl: Filter \nSample #"])

for i in range(len(log)):
    a = log["Log: Chlorophyll Filter Sample #"].iloc[i]
    b = log["Chl: Filter Sample #"].iloc[i]
    if a != b:
        if pd.isnull(a) and pd.isnull(b):
            pass
        else:
            print(str(a) + ": " + str(b))

# Drop unnecessary columns
log.drop(columns=["Chl: Station-Cast #","Chl: Niskin #", "Chl: Filter Sample #"], inplace=True)
log

# **If there is no chlorophyll data yet, need to fill it in with the sample IDs**

# If there is no chlorophyll data yet, need to fill it in 
log["Chl: Chl (ug/l)"] = log["Log: Chlorophyll Filter Sample #"].apply(lambda x: x.replace('-','/') if type(x) == str else x)
log["Chl: Phaeo (ug/l)"] = log["Chl: Chl (ug/l)"]
log["Chl: Comments"] = None

# **===================================================================================================================**
# ### Nutrients

nuts = pd.read_excel(nutrients_path, sheet_name='Summary')

# Rename the columns to name the source
for colname in list(nuts.columns.values):
    nuts.rename(columns={colname: "Nuts: " + colname.replace('Avg: ','')}, inplace=True)
nuts

# Now merge the nutrients data into the sampling log data
log = log.merge(nuts, how="left", left_on=["Log: Nitrate Bottle 1"], right_on=["Nuts: Sample ID"])

# Now check that the merge worked
log[log['Log: Nitrate Bottle 1'].apply(type) != float][["Log: Nitrate Bottle 1","Nuts: Sample ID"]]

# Eliminate the extraneous columns
log.drop(columns=["Nuts: Sample ID"], inplace=True)
log.head()

# **If there is no nutrients data yet, need to fill it in with sample IDs**

nuts_columns = ['Nuts: Nitrate [µmol/L]', 'Nuts: Phosphate [µmol/L]', 'Nuts: Silicate [µmol/L]', 'Nuts: Nitrate+Nitrite [µmol/L]', 'Nuts: Ammonium [µmol/L]','Nuts: Nitrite [µmol/L]']
for col in nuts_columns:
    log[col] = log["Log: Nitrate Bottle 1"]

# **===================================================================================================================**
# ### DIC 

dic = pd.read_excel(dic_path, header=1)
dic.head()

dic['CRUISE_ID']

# Get only the DIC values for a leg of a cruise
# cruise_id = 'AR04-A'
dic = dic[dic['CRUISE_ID'] == cruise_id]
dic

# Select a subset of the columns needed
dic = dic[["CAST_NO", "NISKIN_NO", "DIC_UMOL_KG", "DIC_FLAG_W", "TA_UMOL_KG", "TA_FLAG_W", "PH_TOT_MEA", "TMP_PH_DEG_C", "PH_FLAG_W"]]
# Rename the columns to name the source
for colname in list(dic.columns.values):
    dic.rename(columns={colname: "DIC: " + colname}, inplace=True)
dic.head()

# Merge the carbon data into the sampling log
log = log.merge(dic, how="left", left_on=["Log: Station-Cast #","Log: Niskin #"], right_on=["DIC: CAST_NO", "DIC: NISKIN_NO"])
# log= log.merge(dic, how="left", left_on=["Log: DIC/TA Bottle #"], right_on=["DIC: SAMPLE_ID"])
# Check that the merge worked
# log[log['DIC: CAST_NO'].apply(np.isnan) == False][['Log: Station-Cast #', 'Log: Niskin #', 'DIC: CAST_NO', 'DIC: NISKIN_NO']]

# Drop unnecessary columns
log.drop(columns=["DIC: CAST_NO","DIC: NISKIN_NO"], inplace=True)
# log.drop(columns=["DIC: SAMPLE_ID"], inplace=True)
log.head()

# **If there is no carbon system data yet, need to fill it in with sample IDs**

dic_columns = ["DIC: CAST_NO", "DIC: NISKIN_NO", "DIC: DIC_UMOL_KG", "DIC: DIC_FLAG_W", "DIC: TA_UMOL_KG", "DIC: TA_FLAG_W", "DIC: PH_TOT_MEA", "DIC: TMP_PH_DEG_C", "DIC: PH_FLAG_W"]
for col in dic_columns:
    if 'ph' in col.lower():
        log[col] = log["Log: Ph Bottle #"]
    else:
        log[col] = log["Log: DIC/TA Bottle #"]

# **===================================================================================================================**
# ### CTD Data

# Load the CTD Data
ctd = pd.read_csv(sample_dir + 'CTD_Summary.csv')
ctd.drop(columns='Unnamed: 0', inplace=True)
# Replace the CTD w/CTD: for easier post-processing
for colname in list(ctd.columns.values):
    ctd.rename(columns={colname: 'CTD: ' + colname.replace('CTD ',"")}, inplace=True)
ctd.head()

ctd["CTD: Cast"] = ctd["CTD: Cast"].apply(lambda x: x.lstrip('0'))

# With the CTD data and the Log Data, I want to merge _only_ on the log columns, because those are the measurements we made. This is because the CTD data is more accurate in the measurements.

df = log.merge(ctd, how="outer", left_on=["Log: Station-Cast #", "Log: Niskin #"], right_on=["CTD: Cast","CTD: Bottle Position"])
df.drop_duplicates(inplace=True)
df.info()

[x for x in df.columns.values if 'duplicate' in x.lower()]

# Select a subset of the available columns which will be kept
columns = ['Log: Cruise ID',
           'Log: Station-Cast #',
           'Log: Target Station',
           'CTD: Start Latitude [degrees]', 
           'CTD: Start Longitude [degrees]',
           'CTD: Start Time [UTC]',
           'Log: Bottom Depth [m]',
           'CTD: Filename',
           'CTD: Bottle Position',
           'CTD: Date Time', 
           'CTD: Pressure, Digiquartz [db]', 
           'CTD: Depth [salt water, m]',
           'CTD: Latitude [deg]', 
           'CTD: Longitude [deg]', 
           'CTD: Temperature [ITS-90, deg C]',
           'CTD: Temperature, 2 [ITS-90, deg C]', 
           'CTD: Conductivity [S/m]', 
           'CTD: Conductivity, 2 [S/m]', 
           'CTD: Salinity, Practical [PSU]', 
           'CTD: Salinity, Practical, 2 [PSU]', 
           'CTD: Oxygen, SBE 43 [ml/l]', 
           'CTD: Oxygen Saturation, Garcia & Gordon [ml/l]', 
           'CTD: Beam Attenuation, WET Labs C-Star [1/m]',
           'CTD: Beam Transmission, WET Labs C-Star [%]', 
           'Oxy: Oxygen [mL/L]', 
           'Log: Oxygen Duplicate',
           'Chl: Chl (ug/l)', 
           'Chl: Phaeo (ug/l)', 
           'Log: Chlorophyll Duplicate', 
           'Log: Chlorophyll Filter Duplicate',
           'Nuts: Nitrate', # [µmol/L]',
           'Nuts: Phosphate',# [µmol/L]',
           'Nuts: Silicate', # [µmol/L]', 
           'Nuts: Nitrate+Nitirte',# [µmol/L]',
           'Nuts: Ammonium',# [µmol/L]', 
           'Nuts: Nitrite',# [µmol/L]', 
           'Log: Nitrate Duplicate',
           'Sal: Salinity [psu]',
           'Log: Salts Duplicate',
           'DIC: TA_UMOL_KG', 
           'DIC: TA_FLAG_W', 
           'DIC: DIC_UMOL_KG', 
           'DIC: DIC_FLAG_W',
           'DIC: PH_TOT_MEA', 
           'DIC: TMP_PH_DEG_C', 
           'DIC: PH_FLAG_W',
           'Log: Comments', 
           'Chl: Comments']

summary = df[columns]
summary = summary.sort_values(by=["Log: Station-Cast #","CTD: Bottle Position"])

summary['Log: Bottom Depth [m]'] = summary['Log: Bottom Depth [m]'].apply(lambda x: x.replace('m','').strip() if type(x) == str else x)

# Now, I need to fill in metadata columns based on other columns to make as complete a dataset as is possible
summary['Log: Station-Cast #'] = summary['Log: Station-Cast #'].fillna(value=df['CTD: Cast'])
summary['CTD: Start Latitude [degrees]'] = summary['CTD: Start Latitude [degrees]'].fillna(value=df['Log: Start Latitude'])
summary['CTD: Start Longitude [degrees]'] = summary['CTD: Start Longitude [degrees]'].fillna(value=df['Log: Start Longitude'])
summary['CTD: Start Time [UTC]'] = summary['CTD: Start Time [UTC]'].fillna(value=df['Log: Start Date'])
summary['CTD: Bottle Position'] = summary['CTD: Bottle Position'].fillna(value=df['Log: Niskin #'])

summary = summary.sort_values(by=['Log: Station-Cast #','CTD: Bottle Position'])
summary

# Create a straight-up mapping instead of this weird fuzzy-wuzzy matching
name_map = {
    'Cruise': 'Log: Cruise ID',
    'Station': 'Log: Station-Cast #',
    'Target Asset': 'Log: Target Station',
    'Start Latitude [degrees]': 'CTD: Start Latitude [degrees]',
    'Start Longitude [degrees]': 'CTD: Start Longitude [degrees]',
    'Start Time [UTC]': 'CTD: Start Time [UTC]',
    'Cast': 'Log: Station-Cast #',
    'Cast Flag': None,
    'Bottom Depth at Start Position [m]': 'Log: Bottom Depth [m]',
    'File': 'CTD: Filename',
    'File Flag': None,
    'Niskin/Bottle Position': 'CTD: Bottle Position',
    'Niskin Flag': None,
    'Bottle Closure Time [UTC]': 'CTD: Date Time',
    'Pressure [db]': 'CTD: Pressure, Digiquartz [db]',
    'Pressure Flag': None,
    'Depth [m]': 'CTD: Depth [salt water, m]',
    'Latitude [deg]': 'CTD: Latitude [deg]',
    'Longitude [deg]': 'CTD: Longitude [deg]',
    'Temperature 1 [deg C]': 'CTD: Temperature [ITS-90, deg C]',
    'Temperature 1 Flag': None,
    'Temperature 2 [deg C]': 'CTD: Temperature, 2 [ITS-90, deg C]',
    'Temperature 2 Flag': None,
    'Conductivity 1 [S/m]': 'CTD: Conductivity [S/m]',
    'Conductivity 1 Flag': None,
    'Conductivity 2 [S/m]': 'CTD: Conductivity, 2 [S/m]',
    'Conductivity 2 Flag': None,
    'Salinity 1, uncorrected [psu]': 'CTD: Salinity, Practical [PSU]',
    'Salinity 2, uncorrected [psu]': 'CTD: Salinity, Practical, 2 [PSU]',
    'Oxygen, uncorrected [mL/L]': 'CTD: Oxygen, SBE 43 [ml/l]',
    'Oxygen Flag': None,
    'Oxygen Saturation [mL/L]': 'CTD: Oxygen Saturation, Garcia & Gordon [ml/l]',
    'Fluorescence [mg/m^3]': None,
    'Fluorescence Flag': None,
    'Beam Attenuation [1/m]': 'CTD: Beam Attenuation, WET Labs C-Star [1/m]',
    'Beam Transmission [%]': 'CTD: Beam Transmission, WET Labs C-Star [%]',
    'Transmissometer Flag': None,
    'pH': None,
    'pH Flag': None,
    'Discrete Oxygen [mL/L]': 'Oxy: Oxygen [mL/L]',
    'Discrete Oxygen Flag': None,
    'Discrete Oxygen Duplicate Flag': 'Log: Oxygen Duplicate',
    'Discrete Chlorophyll [ug/L]': 'Chl: Chl (ug/l)',
    'Discrete Phaeopigment [ug/L]': 'Chl: Phaeo (ug/l)',
    'Discrete Fo/Fa Ratio': None,
    'Discrete Fluorescence Flag': None,
    'Discrete Fluorescence Duplicate Flag': 'Log: Chlorophyll Duplicate',
    'Discrete Phosphate [uM]': 'Nuts: Phosphate', #' [µmol/L]',
    'Discrete Silicate [uM]': 'Nuts: Silicate',# [µmol/L]',
    'Discrete Nitrate [uM]': 'Nuts: Nitrate',# [µmol/L]',
    'Discrete Nitrite [uM]': 'Nuts: Nitrite',# [µmol/L]',
    'Discrete Ammonium [uM]': 'Nuts: Ammonium',# [µmol/L]',
    'Discrete Nutrients Flag': None,
    'Discrete Nutrients Duplicate Flag': 'Log: Nitrate Duplicate',
    'Discrete Salinity [psu]': 'Sal: Salinity [psu]',
    'Discrete Salinity Flag': None,
    'Discrete Salinity Duplicate Flag': 'Log: Salts Duplicate',
    'Discrete Alkalinity [µmol/kg]': 'DIC: TA_UMOL_KG',
    'Discrete Alkalinity Flag': 'DIC: TA_FLAG_W',
    'Discrete DIC [µmol/kg]': 'DIC: DIC_UMOL_KG',
    'Discrete DIC Flag': 'DIC: DIC_FLAG_W',
    'Discrete pCO2 [µatm]': None, 
    'Discrete pCO2 Analysis Temp [C]': None,
    'Discrete pCO2 Flag': None,
    'Discrete pH [Total scale]': 'DIC: PH_TOT_MEA',
    'Discrete pH Analysis Temp [C]': 'DIC: TMP_PH_DEG_C',
    'Discrete pH Flag': 'DIC: PH_FLAG_W',
    'Calculated Alkalinity [µmol/kg]': None,
    'Calculated DIC [µmol/kg]': None,
    'Calculated pCO2 [µatm]': None,
    'Calculated pH': None,
    'Calculated CO2aq [µmol/kg]': None,
    'Calculated bicarb [µmol/kg]': None,
    'Calculated CO3 [µmol/kg]': None,
    'Calculated Omega-C': None,
    'Calculated Omega-A': None,
    'Comments': 'Log: Comments',
    'Chl Comments': 'Chl: Comments'
}

final = pd.DataFrame()
for key in name_map:
    column = name_map.get(key)
    if column is not None:
        final[key] = summary[column]
    else:
        final[key] = None

final.sort_values(by=['Cruise','Station','Niskin/Bottle Position'], inplace=True)

final

final.drop_duplicates(inplace=True)
final

cruise_id

# cruise_id = 'AR-31A'
final['Cruise'] = final['Cruise'].fillna(value=cruise_id)

cruise_name = cruise.replace('/','').split('_')[0]
current_date = pd.to_datetime(pd.datetime.now()).tz_localize(tz='US/Eastern').tz_convert(tz='UTC')
version = '1-2'

cruise_id, cruise_name

filename = '_'.join([cruise_name,cruise_id,'Discrete','Summary',current_date.strftime('%Y-%m-%d'),'ver',version,'.csv'])
filename

final.fillna(value=-9999999,inplace=True)

final.to_csv(basepath+array+cruise+filename, index=False)



# ## Update summary sheets
#

basepath = '/home/andrew/Documents/OOI-CGSN/QAQC_Sandbox/Ship_data/'

os.listdir(basepath+'Pioneer')

basepath = '/home/andrew/Documents/OOI-CGSN/QAQC_Sandbox/Ship_data/'
array = 'Pioneer/'
cruise =   'Pioneer-08_AR-18_2017-05-30/'

sorted(os.listdir(basepath+array+cruise))

water = 'Water Sampling/'
ctd = 'ctd/'
leg = 'Leg 1 (ar08a)/'

sorted(os.listdir(basepath + array + cruise + water))

sample_dir = basepath + array + cruise + ctd
water_dir = basepath + array + cruise + water
log_path = water_dir + 'Pioneer-08_AR-18A_CTD_Sampling_Log.xlsx'
chl_path = water_dir + ''
nutrients_path = water_dir + 'Pioneer-08_AR-18_DIC_Sample_Data_2019-11-06_ver_1-00.xlsx'
salts_and_o2_path = water_dir + 'Pioneer-08_AR-18A_Oxygen_Salinity_Sample_Data/'


data_path = basepath + array + cruise + 'Pioneer-08_AR-18_Discrete_Summary_2019-11-04_ver_1-02_.csv'

data = pd.read_csv(data_path)
data

data.columns

# Load the relevant data
dic_path = basepath + array + cruise + water + 'Pioneer-08_AR-18_DIC_Sample_Data_2019-11-06_ver_1-00.xlsx'
dic = pd.read_excel(dic_path)
dic.head()


def reformat_cruise_id(x):
    if '-A' in x:
        x = 'AR-18A'
    elif '-B' in x:
        x = 'AR-18B'
    elif '-C' in x:
        x = 'AR-18C'
    else:
        x
    return x


dic["CRUISE_ID"] = dic["CRUISE_ID"].apply(lambda x: reformat_cruise_id(x) )

df = data.merge(dic, how='left', left_on=['Cruise', 'Station', 'Niskin/Bottle Position'], right_on = ["CRUISE_ID", "CAST_NO", "NISKIN_NO"])

df["Discrete DIC [µmol/kg]"] = df["DIC_UMOL_KG"]
df["Discrete DIC Flag"] = df['DIC_FLAG_W']
df["Discrete Alkalinity [µmol/kg]"] = df["TA_UMOL_KG"]
df["Discrete Alkalinity Flag"] = df['TA_FLAG_W']
df["Discrete pH [Total scale]"] = df['PH_TOT_MEA']
df["Discrete pH Analysis Temp [C]"] = df["TMP_PH_DEG_C"]
df["Discrete pH Flag"] = df["PH_FLAG_W"]

df[df["Discrete pH [Total scale]"].notna()]["Discrete pH [Total scale]"]

df.drop(columns=[x for x in dic.columns], inplace=True)

df.columns

df.fillna(value=-9999999,inplace=True)


def fill_flags(x):
    if x == -9999999 or x == '-9999999':
        return x
    elif str(x).lower() == 'false':
        return x
    elif str(x).lower() == 'true':
        return x
    else:
        return str(x).zfill(16)


for column in df.columns:
    if 'flag' in column.lower():
        df[column] = df[column].apply(lambda x: fill_flags(x))

df

df.to_csv(basepath+array+cruise+'Pioneer-08_AR-18_Discrete_Summary_2019-11-07_ver_1-03_.csv', index=False)

data


