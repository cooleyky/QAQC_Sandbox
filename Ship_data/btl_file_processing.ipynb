{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottle Processing\n",
    "Author: Andrew Reed\n",
    "\n",
    "### Motivation:\n",
    "Independent verification of the suite of physical and chemical observations provided by OOI are critical for the observations to be of use for scientifically valid investigations. Consequently, CTD casts and Niskin water samples are made during deployment and recovery of OOI platforms, vehicles, and instrumentation. The water samples are subsequently analyzed by independent labs for  comparison with the OOI telemetered and recovered data.\n",
    "\n",
    "However, currently the water sample data routinely collected and analyzed as part of the OOI program are not available in a standardized format which maps the different chemical analyses to the physical measurements taken at bottle closure. Our aim is to make these physical and chemical analyses of collected water samples available to the end-user in a standardized format for easy comprehension and use, while maintaining the source data files. \n",
    "\n",
    "### Approach:\n",
    "Generating a summary of the water sample analyses involves preprocessing and concatenating multiple data sources, and accurately matching samples with each other. To do this, I first preprocess the ctd casts to generate bottle (.btl) files using the SeaBird vendor software following the SOP available on Alfresco. \n",
    "\n",
    "Next, the bottle files are parsed using python code and the data renamed following SeaBird's naming guide. This creates a series of individual cast summary (.sum) files. These files are then loaded into pandas dataframes, appended to each other, and exported as a csv file containing all of the bottle data in a single data file.\n",
    "\n",
    "### Data Sources/Software:\n",
    "\n",
    "* **sbe_name_map**: This is a spreadsheet which maps the short names generated by the SeaBird SBE DataProcessing Software to the associated full names. The name mapping originates from SeaBird's SBE DataProcessing support documentation.\n",
    "\n",
    "* **Alfresco**: The Alfresco CMS for OOI at alfresco.oceanobservatories.org is the source of the ctd hex, xmlcon, and psa files necessary for generating the bottle files needed to create the sample summary sheet.\n",
    "\n",
    "* **SBEDataProcessing-Win32**: SeaBird vendor software for processing the raw ctd files and generating the .btl files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages used in this notebook\n",
    "import os, sys, re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the name mapping for the column names\n",
    "# Specifiy the local directory\n",
    "sbe_name_map = pd.read_excel('/media/andrew/OS/Users/areed/Documents/OOI-CGSN/QAQC_Sandbox/Reference_Files/seabird_ctd_name_map.xlsx')\n",
    "# sbe_name_map = pd.read_excel('C:/Users/areed/Documents/OOI-CGSN/QAQC_Sandbox/Reference_Files/seabird_ctd_name_map.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/media/andrew/OS/Users/areed/Documents/OOI-CGSN/QAQC_Sandbox/Ship_data/'\n",
    "array = 'Pioneer/'\n",
    "cruise = 'Pioneer-05/Leg_1/'\n",
    "water_dir = 'Water Sampling/'\n",
    "ctd_dir = 'ctd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the local directory where the bottle (.btl) files are stored for a particular cruise\n",
    "# dirpath = 'C:/Users/areed/Documents/OOI-CGSN/QAQC_Sandbox/Ship_data/Irminger/Irminger-5/ctd/'\n",
    "btlpath = basepath+array+cruise+ctd_dir\n",
    "summary_sheet_path = basepath+array+cruise+water_dir+'Pioneer2_KN-217_sampling_log-1.xlsx'\n",
    "salts_and_o2_path = basepath+array+cruise+water_dir+'Salts and O2/'\n",
    "nutrients_path = basepath+array+cruise+water_dir+'Pioneer II Nutrient Data 2014.xlsx'\n",
    "chl_path = basepath+array+cruise+water_dir+'CHLs KN-214.xls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at31005.hdr',\n",
       " 'at990.bl',\n",
       " '09P-0749_at31abc.xmlcon',\n",
       " 'at31001.bl',\n",
       " 'at31001.btl',\n",
       " 'at31001.hdr',\n",
       " 'at31001.hex',\n",
       " 'at31001.ros',\n",
       " 'at31001.sum',\n",
       " 'AT31001.XMLCON',\n",
       " 'at31002.bl',\n",
       " 'at31002.btl',\n",
       " 'at31002.hdr',\n",
       " 'at31002.hex',\n",
       " 'at31002.ros',\n",
       " 'at31002.sum',\n",
       " 'AT31002.XMLCON',\n",
       " 'at31003.bl',\n",
       " 'at31003.btl',\n",
       " 'at31003.hdr',\n",
       " 'at31003.hex',\n",
       " 'at31003.ros',\n",
       " 'at31003.sum',\n",
       " 'AT31003.XMLCON',\n",
       " 'at31004.bl',\n",
       " 'at31004.btl',\n",
       " 'at31004.hdr',\n",
       " 'at31004.hex',\n",
       " 'at31004.ros',\n",
       " 'at31004.sum',\n",
       " 'AT31004.XMLCON',\n",
       " 'at31005.bl',\n",
       " 'at31005.btl',\n",
       " 'at31005.hex',\n",
       " 'at31005.ros',\n",
       " 'at31005.sum',\n",
       " 'AT31005.XMLCON',\n",
       " 'at31006.bl',\n",
       " 'at31006.btl',\n",
       " 'at31006.hdr',\n",
       " 'at31006.hex',\n",
       " 'at31006.ros',\n",
       " 'at31006.sum',\n",
       " 'AT31006.XMLCON',\n",
       " 'at31007.bl',\n",
       " 'at31007.btl',\n",
       " 'at31007.hdr',\n",
       " 'at31007.hex',\n",
       " 'at31007.ros',\n",
       " 'at31007.sum',\n",
       " 'AT31007.XMLCON',\n",
       " 'at31008.bl',\n",
       " 'at31008.btl',\n",
       " 'at31008.hdr',\n",
       " 'at31008.hex',\n",
       " 'at31008.ros',\n",
       " 'at31008.sum',\n",
       " 'AT31008.XMLCON',\n",
       " 'at31009.bl',\n",
       " 'at31009.hdr',\n",
       " 'at31009.hex',\n",
       " 'AT31009.XMLCON',\n",
       " 'at990.hdr',\n",
       " 'at990.hex',\n",
       " 'AT990.XMLCON',\n",
       " 'at991.bl',\n",
       " 'at991.hdr',\n",
       " 'at991.hex',\n",
       " 'AT991.XMLCON',\n",
       " 'at992.bl',\n",
       " 'at992.hdr',\n",
       " 'at992.hex',\n",
       " 'AT992.XMLCON',\n",
       " 'com_port_setup_at31abc.psa',\n",
       " 'CTD_Summary.csv',\n",
       " 'doc',\n",
       " 'fixed_all.dsa',\n",
       " 'fixed_safety.dsa',\n",
       " 'main_display.dsa',\n",
       " 'process',\n",
       " 'Seasave.psa',\n",
       " 'Seasave_std.psa']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(btlpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the data for the start_time\n",
    "def parse_header(header):\n",
    "    \"\"\"\n",
    "    Parse the header of bottle (.btl) files to get critical information\n",
    "    for the summary spreadsheet.\n",
    "    \n",
    "    Args:\n",
    "        header - an object containing the header of the bottle file as a list of\n",
    "            strings, split at the newline.\n",
    "    Returns:\n",
    "        hdr - a dictionary object containing the start_time, filename, latitude,\n",
    "            longitude, and cruise id.\n",
    "    \"\"\"\n",
    "    hdr = {}\n",
    "    for line in header:\n",
    "        if 'start_time' in line.lower():\n",
    "            start_time = pd.to_datetime(re.split('= |\\[',line)[1])\n",
    "            hdr.update({'Start Time':start_time.strftime('%Y-%m-%dT%H:%M:%SZ')})\n",
    "        elif 'filename' in line.lower():\n",
    "            hex_name = re.split('=',line)[1].strip()\n",
    "            hdr.update({'Hex name':hex_name})\n",
    "        elif 'latitude' in line.lower():\n",
    "            start_lat = re.split('=',line)[1].strip()\n",
    "            hdr.update({'Start Latitude':start_lat})\n",
    "        elif 'longitude' in line.lower():\n",
    "            start_lon = re.split('=',line)[1].strip()\n",
    "            hdr.update({'Start Longitude':start_lon})\n",
    "        elif 'cruise id' in line.lower():\n",
    "            cruise_id = re.split(':',line)[1].strip()\n",
    "            hdr.update({'Cruise ID':cruise_id})\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return hdr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write a function to autopopulate the bottle summary sample sheet\n",
    "files = [x for x in os.listdir(btlpath) if '.btl' in x]\n",
    "for filename in files:\n",
    "    filepath = os.path.abspath(btlpath+filename)\n",
    "    \n",
    "    # Load the raw content into memory\n",
    "    with open(filepath) as file:\n",
    "        content = file.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    \n",
    "    # Now parse the file content\n",
    "    header = []\n",
    "    columns = []\n",
    "    data = []\n",
    "    for line in content:\n",
    "        if line.startswith('*') or line.startswith('#'):\n",
    "            header.append(line)\n",
    "        else:\n",
    "            try:\n",
    "                float(line[0])\n",
    "                data.append(line)\n",
    "            except:\n",
    "                columns.append(line)\n",
    "    \n",
    "    # Parse the header\n",
    "    hdr = parse_header(header)\n",
    "    \n",
    "    # Parse the column identifiers\n",
    "    column_dict = {}\n",
    "    for line in columns:\n",
    "        for i,x in enumerate(line.split()):\n",
    "            try:\n",
    "                column_dict[i] = column_dict[i] + ' ' + x\n",
    "            except:\n",
    "                column_dict.update({i:x})\n",
    "    \n",
    "    # Parse the bottle data based on the column header locations\n",
    "    data_dict = {x:[] for x in column_dict.keys()}\n",
    "\n",
    "    for line in data:\n",
    "        if line.endswith('(avg)'):\n",
    "            values = list(filter(None,re.split('  |\\t', line) ) )\n",
    "            for i,x in enumerate(values):\n",
    "                data_dict[i].append(x)\n",
    "        elif line.endswith('(sdev)'):\n",
    "            values = list(filter(None,re.split('  |\\t', line) ) )\n",
    "            data_dict[1].append(values[0])\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "    data_dict[1] = [' '.join(item) for item in zip(data_dict[1][::2],data_dict[1][1::2])]\n",
    "    \n",
    "    # With the parsed data and column names, match up the data and column\n",
    "    # based on the location\n",
    "    results = {}\n",
    "    for key,item in column_dict.items():\n",
    "        values = data_dict[key]\n",
    "        results.update({item:values})\n",
    "        \n",
    "    # Put the results into a dataframe\n",
    "    df = pd.DataFrame.from_dict(results)\n",
    "        \n",
    "    # Now add the parsed info from the header files into the dataframe\n",
    "    for key,item in hdr.items():\n",
    "        df[key] = item\n",
    "        \n",
    "    # Get the cast number\n",
    "    cast = filename[filename.index('.')-3:filename.index('.')]\n",
    "    df['Cast #'] = str(cast).zfill(3)\n",
    "    \n",
    "    # Generate a filename for the summary file\n",
    "    outname = filename.split('.')[0] + '.sum'\n",
    "    \n",
    "    # Save the results\n",
    "    df.to_csv(btlpath+outname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hex name': 'C:\\\\data\\\\ctd\\\\at31008.hex',\n",
       " 'Start Latitude': '40 21.90 N',\n",
       " 'Start Longitude': '070 46.50 W',\n",
       " 'Start Time': '2015-10-16T14:35:11Z'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for each \"summary\" file, load and append to each other\n",
    "df = pd.DataFrame()\n",
    "for file in os.listdir(btlpath):\n",
    "    if '.sum' in file:\n",
    "        df = df.append(pd.read_csv(btlpath+file))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Bottle Position</th>\n",
       "      <th>Date Time</th>\n",
       "      <th>PrDM</th>\n",
       "      <th>DepSM</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>T090C</th>\n",
       "      <th>T190C</th>\n",
       "      <th>C0S/m</th>\n",
       "      <th>...</th>\n",
       "      <th>Sal11</th>\n",
       "      <th>Sbeox0ML/L</th>\n",
       "      <th>OxsolML/L</th>\n",
       "      <th>CStarAt0</th>\n",
       "      <th>CStarTr0</th>\n",
       "      <th>Hex name</th>\n",
       "      <th>Start Latitude</th>\n",
       "      <th>Start Longitude</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Cast #</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Oct 12 2015 16:39:05</td>\n",
       "      <td>443.326</td>\n",
       "      <td>439.460</td>\n",
       "      <td>39.94014</td>\n",
       "      <td>-70.8834</td>\n",
       "      <td>6.0237</td>\n",
       "      <td>6.0239</td>\n",
       "      <td>3.463375</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0512</td>\n",
       "      <td>4.4897</td>\n",
       "      <td>6.90263</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>94.3332 (avg)</td>\n",
       "      <td>C:\\data\\ctd\\at31001.hex</td>\n",
       "      <td>39 56.41 N</td>\n",
       "      <td>070 53.00 W</td>\n",
       "      <td>2015-10-12T15:23:02Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Oct 12 2015 16:39:23</td>\n",
       "      <td>443.278</td>\n",
       "      <td>439.413</td>\n",
       "      <td>39.94014</td>\n",
       "      <td>-70.8834</td>\n",
       "      <td>6.0226</td>\n",
       "      <td>6.0234</td>\n",
       "      <td>3.463267</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0512</td>\n",
       "      <td>4.4967</td>\n",
       "      <td>6.90281</td>\n",
       "      <td>0.2632</td>\n",
       "      <td>93.6523 (avg)</td>\n",
       "      <td>C:\\data\\ctd\\at31001.hex</td>\n",
       "      <td>39 56.41 N</td>\n",
       "      <td>070 53.00 W</td>\n",
       "      <td>2015-10-12T15:23:02Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Oct 12 2015 16:45:09</td>\n",
       "      <td>251.878</td>\n",
       "      <td>249.798</td>\n",
       "      <td>39.94014</td>\n",
       "      <td>-70.8834</td>\n",
       "      <td>9.3260</td>\n",
       "      <td>9.3490</td>\n",
       "      <td>3.774700</td>\n",
       "      <td>...</td>\n",
       "      <td>35.1904</td>\n",
       "      <td>2.9037</td>\n",
       "      <td>6.40054</td>\n",
       "      <td>0.1663</td>\n",
       "      <td>95.9263 (avg)</td>\n",
       "      <td>C:\\data\\ctd\\at31001.hex</td>\n",
       "      <td>39 56.41 N</td>\n",
       "      <td>070 53.00 W</td>\n",
       "      <td>2015-10-12T15:23:02Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Oct 12 2015 16:45:17</td>\n",
       "      <td>252.137</td>\n",
       "      <td>250.054</td>\n",
       "      <td>39.94014</td>\n",
       "      <td>-70.8834</td>\n",
       "      <td>9.3717</td>\n",
       "      <td>9.3705</td>\n",
       "      <td>3.779986</td>\n",
       "      <td>...</td>\n",
       "      <td>35.1959</td>\n",
       "      <td>2.9237</td>\n",
       "      <td>6.39372</td>\n",
       "      <td>0.1657</td>\n",
       "      <td>95.9419 (avg)</td>\n",
       "      <td>C:\\data\\ctd\\at31001.hex</td>\n",
       "      <td>39 56.41 N</td>\n",
       "      <td>070 53.00 W</td>\n",
       "      <td>2015-10-12T15:23:02Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Oct 12 2015 16:55:13</td>\n",
       "      <td>52.160</td>\n",
       "      <td>51.755</td>\n",
       "      <td>39.94014</td>\n",
       "      <td>-70.8834</td>\n",
       "      <td>21.8404</td>\n",
       "      <td>21.8252</td>\n",
       "      <td>5.073489</td>\n",
       "      <td>...</td>\n",
       "      <td>35.7202</td>\n",
       "      <td>4.7131</td>\n",
       "      <td>4.98466</td>\n",
       "      <td>0.2745</td>\n",
       "      <td>93.3688 (avg)</td>\n",
       "      <td>C:\\data\\ctd\\at31001.hex</td>\n",
       "      <td>39 56.41 N</td>\n",
       "      <td>070 53.00 W</td>\n",
       "      <td>2015-10-12T15:23:02Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Bottle Position             Date Time     PrDM    DepSM  \\\n",
       "0           0                1  Oct 12 2015 16:39:05  443.326  439.460   \n",
       "1           1                2  Oct 12 2015 16:39:23  443.278  439.413   \n",
       "2           2                3  Oct 12 2015 16:45:09  251.878  249.798   \n",
       "3           3                4  Oct 12 2015 16:45:17  252.137  250.054   \n",
       "4           4                5  Oct 12 2015 16:55:13   52.160   51.755   \n",
       "\n",
       "   Latitude  Longitude    T090C    T190C     C0S/m  ...      Sal11  \\\n",
       "0  39.94014   -70.8834   6.0237   6.0239  3.463375  ...    35.0512   \n",
       "1  39.94014   -70.8834   6.0226   6.0234  3.463267  ...    35.0512   \n",
       "2  39.94014   -70.8834   9.3260   9.3490  3.774700  ...    35.1904   \n",
       "3  39.94014   -70.8834   9.3717   9.3705  3.779986  ...    35.1959   \n",
       "4  39.94014   -70.8834  21.8404  21.8252  5.073489  ...    35.7202   \n",
       "\n",
       "   Sbeox0ML/L  OxsolML/L  CStarAt0       CStarTr0                 Hex name  \\\n",
       "0      4.4897    6.90263    0.2333  94.3332 (avg)  C:\\data\\ctd\\at31001.hex   \n",
       "1      4.4967    6.90281    0.2632  93.6523 (avg)  C:\\data\\ctd\\at31001.hex   \n",
       "2      2.9037    6.40054    0.1663  95.9263 (avg)  C:\\data\\ctd\\at31001.hex   \n",
       "3      2.9237    6.39372    0.1657  95.9419 (avg)  C:\\data\\ctd\\at31001.hex   \n",
       "4      4.7131    4.98466    0.2745  93.3688 (avg)  C:\\data\\ctd\\at31001.hex   \n",
       "\n",
       "  Start Latitude Start Longitude            Start Time Cast #  \n",
       "0     39 56.41 N     070 53.00 W  2015-10-12T15:23:02Z      1  \n",
       "1     39 56.41 N     070 53.00 W  2015-10-12T15:23:02Z      1  \n",
       "2     39 56.41 N     070 53.00 W  2015-10-12T15:23:02Z      1  \n",
       "3     39 56.41 N     070 53.00 W  2015-10-12T15:23:02Z      1  \n",
       "4     39 56.41 N     070 53.00 W  2015-10-12T15:23:02Z      1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbe_name_map['Short Name'].apply(lambda x: str(x).lower());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column title using the sbe_name_mapping \n",
    "for colname in list(df.columns.values):\n",
    "    try:\n",
    "        fullname = list(sbe_name_map[sbe_name_map['Short Name'].apply(lambda x: str(x).lower() == colname.lower()) == True]['Full Name'])[0]\n",
    "        df.rename({colname:fullname},axis='columns',inplace=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Bottle Position':'Niskin #'},inplace=True)\n",
    "df['Niskin #'] = df['Niskin #'].apply(lambda x: str( int(x) ) )\n",
    "df.drop(columns='Unnamed: 0',inplace=True)\n",
    "df['Cast #'] = df['Cast #'].apply(lambda x: str(x).zfill(3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(btlpath+'CTD_Summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oxygen & Salinity \n",
    "Now, we need to add the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sal_files(dirpath):\n",
    "    \n",
    "    # Run check if files are held in excel format or csvs\n",
    "    csv_flag = any(files.endswith('.SAL') for files in os.listdir(dirpath))\n",
    "    if csv_flag:\n",
    "        for files in os.listdir(dirpath):\n",
    "            sample = []\n",
    "            salinity = []\n",
    "            with open(basepath+array+cruise+'Water Sampling/Salts and O2/'+leg+files) as file:\n",
    "                data = file.readlines()\n",
    "                for ind1,line in enumerate(data):\n",
    "                    if ind1 == 0:\n",
    "                        strs = data[0].replace('\"','').split(',')\n",
    "                        cruisename = strs[0]\n",
    "                        station = strs[1]\n",
    "                        cast = strs[2]\n",
    "                        case = strs[8]\n",
    "                    elif int(line.split()[0]) == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        strs = line.split()\n",
    "                        sample.append(strs[0])\n",
    "                        salinity.append(strs[2]) \n",
    "                # Generate a pandas dataframe to populate data\n",
    "                data_dict = {'Cruise ID':cruisename,'Station #':station,'Cast #':cast,'Case':case,'Sample ID':sample,'Salinity [psu]':salinity}\n",
    "                df = pd.DataFrame.from_dict(data_dict)\n",
    "                df.to_csv(file.name.replace('.','')+'.csv')\n",
    "    \n",
    "    else:\n",
    "        # If the files are already in excel spreadsheets, they've been cleaned into a\n",
    "        # logical tabular format\n",
    "        pass\n",
    "    \n",
    "\n",
    "def process_sal_files(dirpath):\n",
    "    \n",
    "    # Check if the files are excel files or not\n",
    "    excel_flag = any(files.endswith('SAL.xlsx') for files in os.listdir(dirpath))\n",
    "    # Initialize a dataframe for processing the salinity files\n",
    "    df = pd.DataFrame()\n",
    "    if excel_flag:\n",
    "        for file in os.listdir(dirpath):\n",
    "            if 'SAL.xlsx' in file:\n",
    "                df = df.append(pd.read_csv(dirpath+file))\n",
    "        df.rename({'Cruise':'Cruise ID','Station':'Station #','Sample':'Sample ID','Salinity':'Salinity [psu]'},\n",
    "          axis='columns',inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df['Station #'] = df['Station #'].apply(lambda x: str( int(x)).zfill(3))\n",
    "        df['Niskin #'] = df['Niskin #'].apply(lambda x: str( int(x)))\n",
    "        df['Sample ID'] = df['Sample ID'].apply(lambda x: str( int(x)))\n",
    "    else:\n",
    "        for file in os.listdir(dirpath):\n",
    "            if 'SAL.csv' in file:\n",
    "                df = df.append(pd.read_csv(dirpath+file))\n",
    "        df.dropna(inplace=True)\n",
    "        df['Station #'] = df['Station #'].apply(lambda x: str( int(x)).zfill(3))\n",
    "        df['Sample ID'] = df['Sample ID'].apply(lambda x: str( int(x)))\n",
    "        df.drop(columns=[x for x in list(df.columns.values) if 'unnamed' in x.lower()],inplace=True)\n",
    "\n",
    "    # Save the processed summary file for salinity\n",
    "    df.to_csv(dirpath+'SAL_Summary.csv')\n",
    "    \n",
    "    \n",
    "def process_oxy_files(dirpath):\n",
    "    df = pd.DataFrame()\n",
    "    for file in os.listdir(dirpath):\n",
    "        if 'OXY' in file:\n",
    "            df = df.append(pd.read_excel(dirpath+file))\n",
    "    # Rename and clean up the oxygen data to be uniform across data sets\n",
    "    df.rename({'Cruise':'Cruise ID','Station':'Station #','Sample#':'Sample ID','Oxy':'Oxygen [mL/L]','Unit':'Units'},\n",
    "          axis='columns',inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df['Station #'] = df['Station #'].apply(lambda x: str( int(x)).zfill(3))\n",
    "    df['Niskin #'] = df['Niskin #'].apply(lambda x: str( int(x)))\n",
    "    df['Sample ID'] = df['Sample ID'].apply(lambda x: str( int(x)))\n",
    "    df['Cruise ID'] = df['Cruise ID'].apply(lambda x: x.replace('O','0'))\n",
    "    \n",
    "    # Save the processed summary file for oxygen\n",
    "    df.to_csv(dirpath+'OXY_Summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTD Sampling Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(basepath+array+cruise+water_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sheet_path = basepath+array+cruise+water_dir+'PIONEER-5_AT-31B_CTD_sampling_logs.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log = pd.read_excel(summary_sheet_path,sheet_name='Summary',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename the column headers\n",
    "# sample_log.rename(columns=lambda x: 'Sample Log: ' + x.strip(), inplace=True)\n",
    "# sample_log['Sample Log: Niskin #'] = sample_log['Sample Log: Niskin #'].apply(lambda x: int(x.replace('*','')) if type(x) == str else x )\n",
    "# sample_log['Sample Log: Niskin #'] = sample_log['Sample Log: Niskin #'].apply(lambda x: x if np.isnan(x) else str( int(x) ) )\n",
    "# sample_log['Sample Log: Cast #'] = sample_log['Sample Log: Station-Cast #'].apply(lambda x: str(x).zfill(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutrient & Chlorophyll Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(basepath+array+cruise+water_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chlfile = 'Pioneer-5_AT-31B_chlorophyll_analysis.xlsx'\n",
    "nutfile = 'Pioneer-5_AT-31B_Nutrient_Lab_Results.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients = pd.read_excel(basepath+array+cruise+water_dir+nutfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chl = pd.read_excel(basepath+array+cruise+water_dir+chlfile)\n",
    "chl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(basepath+array+cruise+water_dir+'Salts and O2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Salinity and oxygen summaries\n",
    "sal = pd.read_csv(basepath+array+cruise+water_dir+'Salts and O2/AT31-A/SAL_Summary.csv')\n",
    "if 'case' in [x.lower() for x in sal.columns.values]:\n",
    "    sal['Sample ID'] = sal['Case'] + sal['Sample ID'].apply(lambda x: str(x)) \n",
    "oxy = pd.read_csv(basepath+array+cruise+water_dir+'Salts and O2/AT31-A/OXY_Summary.csv')\n",
    "if 'case' in [x.lower() for x in oxy.columns.values]:\n",
    "    oxy['Sample ID'] = oxy['Case'] + oxy['Sample ID'].apply(lambda x: str(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now need to mak\n",
    "sample_log = sample_log.merge(sal[['Station #','Sample ID','Salinity [psu]']], how='left', left_on=['Station-Cast #','Salts Bottle #'], right_on=['Station #','Sample ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.rename({'Salinity [psu]':'Discrete Salinity [psu]'},axis='columns',inplace=True)\n",
    "sample_log.drop(['Station #','Sample ID'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.rename(columns=lambda x: x.strip(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log = sample_log.merge(oxy[['Station #','Sample ID','Oxygen [mL/L]']], how='left', left_on=['Station-Cast #','Oxygen Bottle #'], right_on=['Station #','Sample ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.rename({'Oxygen [mL/L]':'Discrete Oxygen [mL/L]'},axis='columns',inplace=True)\n",
    "sample_log.drop(['Station #','Sample ID'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrients.rename({'index':'Sample ID'},axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log = sample_log.merge(nutrients, how='left', left_on=['Nitrate Bottle 1'], right_on=['Sample ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.rename(columns=lambda x: x.replace('Avg:', 'Discrete'), inplace=True)\n",
    "sample_log.drop(['Sample ID'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the chlorophyll data\n",
    "chl.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chl_df = chl[['Station-Cast #','Brown Bottle #','Chl (ug/l)','Phaeo (ug/l)']]\n",
    "chl_df.rename(columns=lambda x: 'Discrete ' + x, inplace=True)\n",
    "#chl_df.rename({'Discrete quality_flag':'Discrete Chl quality flag'},axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log = sample_log.merge(chl_df, how='left', left_on=['Station-Cast #','Chlorophyll Brown Bottle #'], right_on=['Discrete Station-Cast #','Discrete Brown Bottle #'])\n",
    "sample_log.drop(['Discrete Station-Cast #','Discrete Brown Bottle #'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now load the CTD summary data\n",
    "CTD = pd.read_csv(basepath+array+cruise+'Leg_1/'+ctd_dir+'CTD_Summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = []\n",
    "for name in list(sample_log.columns.values):\n",
    "    if 'Discrete' in name:\n",
    "        column_list.append(name)\n",
    "column_list.append('Station-Cast #')\n",
    "column_list.append('Niskin #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_data = sample_log[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD = CTD.merge(discrete_data, how='left', left_on=['Cast #','Niskin #'], right_on=['Station-Cast #','Niskin #'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.drop(['Unnamed: 0'],axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.fillna(-9999999, inplace=True)\n",
    "CTD['Date Time'] = CTD['Date Time'].apply(lambda x: pd.to_datetime(x).strftime('%Y-%m-%dT%H:%M:%SZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_sheet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_name = summary_sheet_path.replace('sampling_logs.xlsx','Sample_Summary.csv')\n",
    "summary_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTD.to_csv(summary_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrient_bottles = sample_log['Sample Log: Nitrate Bottle 1'].str.split(',').apply(pd.Series, 1).stack()\n",
    "nutrient_bottles.index = nutrient_bottles.index.droplevel(-1)\n",
    "nutrient_bottles.name = 'Nitrate Bottle #'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrient_bottles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the nutrient bottle number back into the sample log, and remove the excess '.'\n",
    "sample_log = sample_log.join(nutrient_bottles)\n",
    "sample_log['Nitrate Bottle #'] = sample_log['Nitrate Bottle #'].apply(lambda x: x.replace('.','') if type(x) == str else x)\n",
    "sample_log['Nitrate Bottle #'] = sample_log['Nitrate Bottle #'].apply(lambda x: x.replace(' ','') if type(x) == str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can add the nutrient bottle data to the sample log before loading\n",
    "sample_log = sample_log.merge(nutrients, how='left', left_on='Nitrate Bottle #', right_on='Sample ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_log.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_columns = ['Sample Log: Cast #','Sample Log: Niskin #','SAL: Salinity','OXY: Oxy','Avg: Nitrate+Nitrite [µmol/L]', 'Avg: Ammonium [µmol/L]',\n",
    "    'Avg: Phosphate [µmol/L]', 'Avg: Silicate [µmol/L]', 'Avg: Nitrite [µmol/L]', 'Avg: Nitrate [µmol/L]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.merge(sample_log[sample_columns], how='left',\n",
    "                  left_on=['Cast #','Niskin #'], right_on=['Sample Log: Cast #','Sample Log: Niskin #'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_unit = '[' + list(set(sal_df['SAL: Unit']) )[0] + ']'\n",
    "oxy_unit = '[' + list(set(oxy_df['OXY: Unit']) )[0] + ']'\n",
    "sal_unit, oxy_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.drop(['Sample Log: Cast #','Sample Log: Niskin #'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.rename(columns=lambda x: x.replace('SAL:','Bottle') + ' ' + sal_unit if 'SAL:' in x else x, inplace=True)\n",
    "result.rename(columns=lambda x: x.replace('OXY:','Bottle') + ' ' + oxy_unit if 'OXY:' in x else x, inplace=True)\n",
    "result.rename(columns=lambda x: x.replace('Avg:','Bottle') if 'Avg:' in x else x, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all of the nans with -999999 and save to a csv\n",
    "result.fillna(str(-9999999),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(salts_and_o2_path+'Irminger-3_Summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
