{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis for Metadata Review in OOI Asset Management System\n",
    "\n",
    "### Motivation:\n",
    "The Asset Management system for OOI is primarly housed on GitHub in a variety of csv files. Until now, the calibration coefficients stored in the csv files have been manually entered. While we have utilized a \"human-in-the-loop\" review approach to catch errors, some errors have slipped through (e.g. truncation of significant figures).\n",
    "\n",
    "### Approach:\n",
    "My goal is to develop an automated approach to catch possible errors which already exist within the asset management system. To accomplish this, I will compare the csv files loaded into the GitHub asset management system with the original vendor files as well as the QCT (quality control testing) documents which capture the coefficients loaded onto the instrument at the time of reception at WHOI from the vendor.\n",
    "\n",
    "### Data Sources:\n",
    "* **GitHub**: CSV files containing the calibration coefficients. Directory organization by sensor+class. The files are named as \"(CGINS)-(sensor+class)-(serial number)-(YYYYMMDD)\" where YYYYMMDD is the calibration date.\n",
    "* **Vault**: Version-controlled storage location of the vendor calibrations, in the Records/Instrument Records/Instrument directories. Within the relevant directory, calibration files are stored as either .cal, .xmlcon, .pdf, or within zipped directories.\n",
    "* **Alfresco**: Version-controlled web-accessed. The calibrations loaded onto the instrument during the initial checkin-in upon receipt (the QCT process) are stored here as either .cap or .txt files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import likely important packages, etc.\n",
    "import sys, os, csv, re\n",
    "from wcmatch import fnmatch\n",
    "import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as et\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficient_name_map = {\n",
    "            'TA0': 'CC_a0',\n",
    "            'TA1': 'CC_a1',\n",
    "            'TA2': 'CC_a2',\n",
    "            'TA3': 'CC_a3',\n",
    "            'CPCOR': 'CC_cpcor',\n",
    "            'CTCOR': 'CC_ctcor',\n",
    "            'CG': 'CC_g',\n",
    "            'CH': 'CC_h',\n",
    "            'CI': 'CC_i',\n",
    "            'CJ': 'CC_j',\n",
    "            'G': 'CC_g',\n",
    "            'H': 'CC_h',\n",
    "            'I': 'CC_i',\n",
    "            'J': 'CC_j',\n",
    "            'PA0': 'CC_pa0',\n",
    "            'PA1': 'CC_pa1',\n",
    "            'PA2': 'CC_pa2',\n",
    "            'PTEMPA0': 'CC_ptempa0',\n",
    "            'PTEMPA1': 'CC_ptempa1',\n",
    "            'PTEMPA2': 'CC_ptempa2',\n",
    "            'PTCA0': 'CC_ptca0',\n",
    "            'PTCA1': 'CC_ptca1',\n",
    "            'PTCA2': 'CC_ptca2',\n",
    "            'PTCB0': 'CC_ptcb0',\n",
    "            'PTCB1': 'CC_ptcb1',\n",
    "            'PTCB2': 'CC_ptcb2',\n",
    "            # additional types for series O\n",
    "            'C1': 'CC_C1',\n",
    "            'C2': 'CC_C2',\n",
    "            'C3': 'CC_C3',\n",
    "            'D1': 'CC_D1',\n",
    "            'D2': 'CC_D2',\n",
    "            'T1': 'CC_T1',\n",
    "            'T2': 'CC_T2',\n",
    "            'T3': 'CC_T3',\n",
    "            'T4': 'CC_T4',\n",
    "            'T5': 'CC_T5',\n",
    "        }\n",
    "\n",
    "o2_coefficients_map = {\n",
    "            'A': 'CC_residual_temperature_correction_factor_a',\n",
    "            'B': 'CC_residual_temperature_correction_factor_b',\n",
    "            'C': 'CC_residual_temperature_correction_factor_c',\n",
    "            'E': 'CC_residual_temperature_correction_factor_e',\n",
    "            'SOC': 'CC_oxygen_signal_slope',\n",
    "            'OFFSET': 'CC_frequency_offset'\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHOI Asset Tracking Spreadsheet\n",
    "First, I want to load and examine exactly what type of data is stored in the WHOI Asset Tracking Spreadsheet and what information it has that may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whoi_asset_tracking(spreadsheet,sheet_name,instrument_class='All',whoi=True,series=None):\n",
    "    \"\"\"\n",
    "    Loads all the individual sensors of a specific instrument class and\n",
    "    series type. Currently applied only for WHOI deployed instruments.\n",
    "    \n",
    "    Args:\n",
    "        spreadsheet - directory path and name of the excel spreadsheet with\n",
    "            the WHOI asset tracking information.\n",
    "        sheet_name - name of the sheet in the spreadsheet to load\n",
    "        instrument_class - the type (i.e. CTDBP, CTDMO, PCO2W, etc). Defaults\n",
    "            to 'All', which will load all of the instruments\n",
    "        whoi - return only whoi instruments? Defaults to True.\n",
    "        series - a specified class of the instrument to load. Defaults to None,\n",
    "            which will load all of the series for a specified instrument class\n",
    "    \"\"\"\n",
    "    \n",
    "    all_sensors = pd.read_excel(spreadsheet,sheet_name=sheet_name,header=1)\n",
    "    # Select a specific class of instruments\n",
    "    if instrument_class == 'All':\n",
    "        inst_class = all_sensors\n",
    "    else:\n",
    "        inst_class  = all_sensors[all_sensors['Instrument\\nClass']==instrument_class]\n",
    "    # Return only the whoi instruments?\n",
    "    if whoi == True:\n",
    "        whoi_insts = inst_class[inst_class['Deployment History'] != 'EA']\n",
    "    else:\n",
    "        whoi_insts = inst_class\n",
    "    # Slect a specific series of the instrument?\n",
    "    if series != None:\n",
    "        instrument = whoi_insts[whoi_insts['Series'] == series]\n",
    "    else:\n",
    "        instrument = whoi_insts\n",
    " \n",
    "    return instrument\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excel_spreadsheet = 'C:/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "excel_spreadsheet = '/media/andrew/OS/Users/areed/Documents/Project_Files/Documentation/System/System Notebook/WHOI_Asset_Tracking.xlsx'\n",
    "sheet_name = 'Sensors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are all the different series of CTDs?\n",
    "PHSEND = whoi_asset_tracking(excel_spreadsheet,sheet_name,instrument_class='PHSEN',whoi=True)\n",
    "PHSEND[PHSEND['Supplier\\nSerial Number']=='P0068']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(CTDBP['Series'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTDBPP = whoi_asset_tracking(excel_spreadsheet,sheet_name,instrument_class='CTDBP',whoi=True,series='P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTDBPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking instrument calibration values\n",
    "After loading the **WHOI Asset Tracking Sheet**, we now have the following critical data for checking calibration information:\n",
    "* Supplier Serial Number - this links back to the original **.cal**, **.xmlcon**, and vendor docs\n",
    "* OOI UID - this is the link between the instrument and the OOINet\n",
    "* QCT Document Number - this number links the instrument to the QCT screen capture of the calibration values loaded onto the instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to load the **CSV** calibration file\n",
    "In order to check that the calibrations in asset management, I have to be able to load the asset management calibration csv files into a dataframe. \n",
    "* First, get all the unique CTDBPCs in Asset Management\n",
    "* Next, parse the csv files in asset management to get the unique instrument serial numbers\n",
    "* With the serial numbers, find the associated instrument calibration csvs\n",
    "* For each calibration csv, load the data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asset_management(instrument,filepath):\n",
    "    \"\"\"\n",
    "    Loads the calibration csv files from a local repository containing\n",
    "    the asset management information.\n",
    "    \n",
    "    Args:\n",
    "        instrument - a pandas dataframe with the asset tracking information\n",
    "            for a specific instrument.\n",
    "        filepath - the directory path pointing to where the csv files are\n",
    "            stored.\n",
    "    Raises:\n",
    "        TypeError - if the instrument input is not a pandas dataframe\n",
    "    Returns:\n",
    "        csv_dict - a dictionary with keys of the UIDs from the instrument dataframe\n",
    "            which correspond to lists of the relevant calibration csv files\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check that the input is a pandas DataFrame\n",
    "    if type(instrument) != pd.core.frame.DataFrame:\n",
    "        raise TypeError()\n",
    "        \n",
    "    uids = sorted( list( set( instrument['UID'] ) ) )\n",
    "    \n",
    "    csv_dict = {}\n",
    "    for uid in uids:\n",
    "        # Get a specified uid from the instrument dataframe\n",
    "        instrument['UID_match'] = instrument['UID'].apply(lambda x: True if uid in x else False)\n",
    "        instrument[instrument['UID_match'] == True]\n",
    "        \n",
    "        # Now, get all the csvs from asset management for a particular UID\n",
    "        csv_files = []\n",
    "        for file in os.listdir(filepath):\n",
    "            if fnmatch.fnmatch(file,'*'+uid+'*'):\n",
    "                csv_files.append(file)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Update the dictionary storing the asset management files for each UID\n",
    "        if len(csv_files) > 0:\n",
    "            csv_dict.update({uid:csv_files})\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return csv_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dict = load_asset_management(CTDBPP,'../../GitHub/OOI-Integration/asset-management/calibration/CTDBPP/')\n",
    "csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to load the all of the csv files based on their UID\n",
    "def load_csv_info(csv_dict,filepath):\n",
    "    \"\"\"\n",
    "    Loads the calibration coefficient information contained in asset management\n",
    "    \n",
    "    Args:\n",
    "        csv_dict - a dictionary which associates an instrument UID to the\n",
    "            calibration csv files in asset management\n",
    "        filepath - the path to the directory containing the calibration csv files\n",
    "    Returns:\n",
    "        csv_cals - a dictionary which associates an instrument UID to a pandas\n",
    "            dataframe which contains the calibration coefficients. The dataframes\n",
    "            are indexed by the date of calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the calibration data into pandas dataframes, which are then placed into\n",
    "    # a dictionary by the UID\n",
    "    csv_cals = {}\n",
    "    for uid in csv_dict:\n",
    "        cals = pd.DataFrame()\n",
    "        for file in csv_dict[uid]:\n",
    "            data = pd.read_csv(filepath+file)\n",
    "            date = file.split('__')[1].split('.')[0]\n",
    "            data['CAL DATE'] = pd.to_datetime(date)\n",
    "            cals = cals.append(data)\n",
    "        csv_cals.update({uid:cals})\n",
    "        \n",
    "    # Pivot the dataframe to be sorted based on calibration date\n",
    "    for uid in csv_cals:\n",
    "        csv_cals[uid] = csv_cals[uid].pivot(index=csv_cals[uid]['CAL DATE'], columns='name')['value']\n",
    "        \n",
    "    return csv_cals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = load_csv_info(csv_dict,'../../GitHub/OOI-Integration/asset-management/calibration/CTDBPP/')\n",
    "CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have successfully loaded the csv calibrations into a pandas dataframe that allows for easy comparison between calibrations based on the calibration date for each calibration coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the QCT values\n",
    "The next step is to take the capture files from the QCT and load them into a comparable pandas dataframe. This involves several steps:\n",
    "* Get the QCT document numbers from the WHOI Asset Tracking Sheet for each individual instrument\n",
    "* Find where the QCT documents are stored\n",
    "* Load the QCT documents\n",
    "* Parse the QCT documents\n",
    "* Translate the parsed QCT values into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = sorted( list( set( CTDBPP['UID'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict = {}\n",
    "for uid in uids:\n",
    "    # Get the QCT Document numbers from the asset tracking sheet\n",
    "    CTDBPP['UID_match'] = CTDBPP['UID'].apply(lambda x: True if uid in x else False)\n",
    "    qct_series = CTDBPP[CTDBPP['UID_match'] == True]['QCT Testing']\n",
    "    qct_series = list(qct_series.iloc[0].split('\\n'))\n",
    "    qct_dict.update({uid:qct_series})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dirpath = 'C:/Users/areed/Documents/Project_Files/Records/Instrument_Records/cap_files/'\n",
    "dirpath = '/media/andrew/OS/Users/areed/Documents/Project_Files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try building a function to do the file path generator\n",
    "def generate_file_path(dirpath,filename,ext,exclude=['_V','_Data_Workshop']):\n",
    "    \"\"\"\n",
    "    Function which searches for the location of the given file and returns\n",
    "    the full path to the file.\n",
    "    \n",
    "    Args:\n",
    "        dirpath - parent directory path under which to search\n",
    "        filename - the name of the file to search for\n",
    "        ext - \n",
    "        exclude - optional list which allows for excluding certain\n",
    "            directories from the search\n",
    "    Returns:\n",
    "        fpath - the file path to the filename from the current\n",
    "            working directory.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(dirpath):\n",
    "        dirs[:] = [d for d in dirs if d not in exclude]\n",
    "        for fname in files:\n",
    "            if fnmatch.fnmatch(fname, [filename+'*.cap', filename+'*.txt', filename+'*.log']):\n",
    "                fpath = os.path.join(root, fname)\n",
    "                return fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to develop an automated approach to load all the QCT documents, parse them\n",
    "# into a dictionary, and convert the dictionary into a pandas dataframe\n",
    "def load_qct_data(qct_dict,coefficient_name_map,dirpath='../../../Documents/Project_Files/'):\n",
    "    qct = {}\n",
    "    qct_missing = {}\n",
    "    for uid in qct_dict:\n",
    "        print(uid)\n",
    "        capture_data = {}\n",
    "        missing = []\n",
    "        for capfile in qct_dict[uid]:\n",
    "            # First, find and return the path to the capture file which\n",
    "            # matches the capture file indentifier\n",
    "            cappath = generate_file_path(dirpath, capfile)\n",
    "            \n",
    "            # Function to pull out the coefficients from the capture files. This is a naive implementation\n",
    "            # and splits only on either a \":\" or \"=\", it doesn't do any comprehension of the file\n",
    "            if cappath is None:\n",
    "                missing.append(capfile)\n",
    "            else:\n",
    "                coeffs = {}\n",
    "                with open(cappath) as filename:\n",
    "                    data = filename.read()\n",
    "                    for line in data.splitlines():\n",
    "                        items = re.split(': | =',line)\n",
    "                        key = items[0].strip()\n",
    "                        value = items[-1].strip()\n",
    "                        coeffs.update({key:value})\n",
    "                    \n",
    "                # The best way to do this is to use the CTD name mapping to only get the important values\n",
    "                capture = {}\n",
    "                # With the capture coefficients, now map it to the CTD coefficients\n",
    "                for key in coeffs.keys():\n",
    "                    if key in coefficient_name_map.keys():\n",
    "                        capture[coefficient_name_map[key]] = coeffs[key]\n",
    "            \n",
    "                # Get the calibration date\n",
    "                caldate = coeffs['conductivity']\n",
    "            \n",
    "                # Update the capture file to include the calibration date\n",
    "                capture['CAL DATE'] = pd.to_datetime(caldate)\n",
    "            \n",
    "                # Now, update the parent dictionary\n",
    "                capture_data.update({capfile:capture})\n",
    "            \n",
    "        df = pd.DataFrame.from_dict({i: capture_data[i] for i in capture_data.keys()}, orient='index')\n",
    "        qct.update({uid:df})\n",
    "        qct_missing.update({uid:missing})\n",
    "        \n",
    "    return qct, qct_missing   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct, qct_missing = load_qct_data(qct_dict,coefficient_name_map,dirpath='../../../../Documents/Project_Files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index to the calibration date\n",
    "for uid in qct:\n",
    "    qct[uid].set_index('CAL DATE', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vendor Calibration values: .cal and .xmlcon\n",
    "This next step is to load the CTD .cal and .xmlcon files in order to compare the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_serial_num(df):\n",
    "    serial_num = list(df[df['UID_match'] == True]['Supplier\\nSerial Number'])\n",
    "    serial_num = serial_num[0].split('-')[1]\n",
    "    return serial_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums = {}\n",
    "for uid in uids:\n",
    "    CTDBPP['UID_match'] = CTDBPP['UID'].apply(lambda x: True if uid in x else False)\n",
    "    serial_num = get_serial_num(CTDBPP)\n",
    "    serial_nums.update({uid:serial_num})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serial_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cal(data, coefficient_name_map):\n",
    "    \"\"\"\n",
    "    Reads in the calibration coefficients from the vendor supplied\n",
    "    .cal file.\n",
    "        \n",
    "    Args:\n",
    "        self - the CTD object\n",
    "        data - an opened, read cal file that has been interpreted\n",
    "        into ASCII.\n",
    "    Returns:\n",
    "        A populated CTD object's dictionary with coeff names and\n",
    "        associated values from the cal file. \n",
    "    \"\"\"\n",
    "    coefficients = {}\n",
    "    for line in data.splitlines():\n",
    "        key, value = line.replace(\" \",\"\").split('=')\n",
    "\n",
    "        if key == 'INSTRUMENT_TYPE' and value == 'SEACATPLUS':\n",
    "            serial = '16-'\n",
    "\n",
    "        if key == 'SERIALNO':\n",
    "            serial = serial + value\n",
    "    \n",
    "        if key == 'CCALDATE':\n",
    "            date = datetime.datetime.strptime(value, '%d-%b-%y').strftime('%Y%m%d')\n",
    "\n",
    "        name = coefficient_name_map.get(key)\n",
    "        if not name or name is None:\n",
    "            continue\n",
    "        else:\n",
    "            coefficients[name] = value\n",
    "            \n",
    "    return coefficients,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_xml(data, coefficient_name_map, o2_coefficients_map):\n",
    "    Tflag = False\n",
    "    O2flag = False\n",
    "    coefficients = {}\n",
    "    date = None\n",
    "        \n",
    "    for child in data.iter():\n",
    "        key = child.tag.upper()\n",
    "        value = child.text.upper()\n",
    "        \n",
    "        # Do a couple of checks for type of CTD and flag for presence of\n",
    "        # Oxygen sensor, Type (16+ vs 37)\n",
    "        if key == 'OXYGENSENSOR':\n",
    "            O2flag = True\n",
    "        \n",
    "        if key == 'CALIBRATIONDATE':\n",
    "            if date is None and value is not None:\n",
    "                date = datetime.datetime.strptime(value, '%d-%b-%y').strftime('%Y%m%d')\n",
    "            \n",
    "        # Have to rename the temperature keys to 'T'+key because fuck it, nothing is straightforward\n",
    "        if key == 'TEMPERATURESENSOR':\n",
    "            Tflag = True\n",
    "        elif 'SENSOR' in key and Tflag == True:\n",
    "            Tflag = False\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if Tflag == True:\n",
    "            key = 'T'+key\n",
    "        \n",
    "        # Find the mapping of the vendor coeff name -> UFrame coefficient name\n",
    "        try:\n",
    "            name = coefficient_name_map.get(key)\n",
    "        except:\n",
    "            if O2flag == True:\n",
    "                try:\n",
    "                    name = o2_coefficients_map.get(key)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        # Now, can update a dictionary to store key->value pairs of coefficients from the xmlcon file    \n",
    "        coefficients.update({name:value})\n",
    "        \n",
    "    return coefficients,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_files = {}\n",
    "for uid,sn in serial_nums.items():\n",
    "    files = []\n",
    "    for file in os.listdir('../../../../Documents/Project_Files/Records/Instrument_Records/CTDBP/'):\n",
    "        if sn in file:\n",
    "            if 'Calibration_File' in file:\n",
    "                files.append(file)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "    vendor_files.update({uid:files})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cal_coeffs(files, filepath, coefficient_name_map, o2_coefficients_map):\n",
    "    \"\"\"\n",
    "    Loads all of the calibration coefficients from the vendor cal files for\n",
    "    a given CTD instrument class.\n",
    "    \n",
    "    Args:\n",
    "        files - a list of zipfile names containing the vendor calibration files\n",
    "        filepath - directory path to where the zipfiles are stored locally\n",
    "        coefficient_name_map - a mapping of the calibration names in the vendor file\n",
    "            to the calibration coeff names needed for OOINet\n",
    "        o2_coefficients_map - mapping for CTDs containing an oxygen sensor\n",
    "    Returns:\n",
    "        cal_coeffs - a dictionary of the calibration coefficients with the respective\n",
    "            values, nested in a dictionary sorted by calibration date\n",
    "    \"\"\"\n",
    "    cal_coeffs = {}\n",
    "    missing = []\n",
    "    for file in files:\n",
    "        fpath = filepath+file\n",
    "        # If it is a zipfile, unzip to memory, find\n",
    "        if fpath.endswith('.zip'):\n",
    "            with ZipFile(fpath) as zfile:\n",
    "                fname = [name for name in zfile.namelist() if '.cal' in name]\n",
    "                if len(fname) > 0:\n",
    "                    data = zfile.read(fname[0]).decode('ASCII')\n",
    "                    coeffs, date = read_cal(data, coefficient_name_map)\n",
    "                    cal_coeffs.update({date:coeffs})\n",
    "                else:\n",
    "                    print(f\"No vendor documents of type '.cal' found for file {file}.\")\n",
    "                    missing.append(file)\n",
    "        elif fpath.endswith('.cal'):\n",
    "            with open(fpath) as cfile:\n",
    "                data = cfile.read()\n",
    "                coeffs, date = read_cal(data, coefficient_name_map)\n",
    "                cal_coeffs.update({date:coeffs})\n",
    "        else:\n",
    "            print(f\"No vendor documents of type '.cal' found for file {file}.\")\n",
    "            missing.append(file)\n",
    "    \n",
    "    return cal_coeffs, missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = {}\n",
    "cal_missing = {}\n",
    "filepath = '../../../../Documents/Project_Files/Records/Instrument_Records/CTDBP/'\n",
    "for uid,files in vendor_files.items():\n",
    "    cal_coeffs, missing = load_cal_coeffs(files,filepath,coefficient_name_map,o2_coefficients_map)\n",
    "    cal_df = pd.DataFrame.from_dict({i: cal_coeffs[i] for i in cal_coeffs.keys()}, orient='index')\n",
    "    cal_df.index = pd.to_datetime(cal_df.index)\n",
    "    cal.update({uid:cal_df})\n",
    "    cal_missing.update({uid:missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the above process with the .xmlcon file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xml_coeffs(files,filepath, coefficient_name_map, o2_coefficients_map):\n",
    "    \"\"\"\n",
    "    Loads all of the calibration coefficients from the vendor cal files in xmlcon\n",
    "    format for a given CTD instrument class.\n",
    "    \n",
    "    Args:\n",
    "        files - a list of zipfile names containing the vendor calibration files\n",
    "        filepath - directory path to where the zipfiles are stored locally\n",
    "        coefficient_name_map - a mapping of the calibration names in the vendor file\n",
    "            to the calibration coeff names needed for OOINet\n",
    "        o2_coefficients_map - mapping for CTDs containing an oxygen sensor\n",
    "    Returns:\n",
    "        cal_coeffs - a dictionary of the calibration coefficients with the respective\n",
    "            values, nested in a dictionary sorted by calibration date\n",
    "    \"\"\"\n",
    "    \n",
    "    cal_coeffs = {}\n",
    "    missing = []\n",
    "    for file in files:\n",
    "        fpath = filepath+file\n",
    "        # If it is a zipfile, unzip to memory, find\n",
    "        if fpath.endswith('.zip'):\n",
    "            with ZipFile(fpath) as zfile:\n",
    "                fname = [name for name in zfile.namelist() if '.xmlcon' in name]\n",
    "                if len(fname) > 0:\n",
    "                    data = et.parse(zfile.open(fname[0]))\n",
    "                    coeffs, date = read_xml(data, coefficient_name_map, o2_coefficients_map)\n",
    "                    cal_coeffs.update({date:coeffs})\n",
    "                else:\n",
    "                    print(f\"No vendor documents of type '.xmlcon' found for file {file}.\")\n",
    "                    missing.append(file)\n",
    "        elif fpath.endswith('.xmlcon'):\n",
    "            with open(fpath) as xfile:\n",
    "                data = et.parse(xfile)\n",
    "                coeffs, date = read_xml(data, coefficient_name_map, o2_coefficients_map)\n",
    "                cal_coeffs.update({date:coeffs})\n",
    "        else:\n",
    "            print(f\"No vendor documents of type '.xmlcon' found for file {file}.\")\n",
    "            missing.append(file)\n",
    "            \n",
    "    return cal_coeffs, missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = {}\n",
    "xml_missing = {}\n",
    "filepath = '../../../../Documents/Project_Files/Records/Instrument_Records/CTDBP/'\n",
    "for uid,files in vendor_files.items():\n",
    "    xml_coeffs, missing = load_xml_coeffs(files,filepath,coefficient_name_map,o2_coefficients_map)\n",
    "    xml_df = pd.DataFrame.from_dict({i: xml_coeffs[i] for i in xml_coeffs.keys()}, orient='index')\n",
    "    xml_df.drop(columns=[None],axis=1,inplace=True)\n",
    "    xml_df.index = pd.to_datetime(xml_df.index)\n",
    "    xml.update({uid:xml_df})\n",
    "    xml_missing.update({uid:missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons\n",
    "Now that I have .cal, .xmlcon, the qct capture files, and the csv files from asset management, I can begin comparison of the calibration coefficients between the different files. The goal is that the dates, values, and coefficients all match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I need to reindex all of the different dataframes such that they all have two indices:\n",
    "# A dataset index and a datetime index, and set them to uniform name (for concatenation)\n",
    "for uid in uids:\n",
    "    try:\n",
    "        CSV[uid]['Dataset'] = 'CSV'\n",
    "        CSV[uid].set_index(['Dataset',CSV[uid].index],inplace=True)\n",
    "        CSV[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    qct[uid]['Dataset'] = 'QCT'\n",
    "    qct[uid].set_index(['Dataset',qct[uid].index],inplace=True)\n",
    "    qct[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "qct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    cal[uid]['Dataset'] = 'CAL'\n",
    "    cal[uid].set_index(['Dataset',cal[uid].index],inplace=True)\n",
    "    cal[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "cal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    xml[uid]['Dataset'] = 'XML'\n",
    "    xml[uid].set_index(['Dataset',xml[uid].index],inplace=True)\n",
    "    xml[uid].index.set_names(['Dataset','Cal Date'],inplace=True)\n",
    "xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four possible sources of calibration coefficients available for an instrument - the calibration **CSV** loaded into asset management, the calibration coefficients loaded onto the instrument during check-in (**QCT**), the **.cal** file provided by the vendor, and the **XML** file provided by the vendor. \n",
    "\n",
    "The next step is to concatenate the different instruments into a single dataframe and to sort by calibration date. This will allow for comparison based on the date of the calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = {}\n",
    "for uid in uids:\n",
    "    comparison.update({uid:pd.concat([CSV.get(uid), cal.get(uid), xml.get(uid), qct.get(uid)])})\n",
    "    comparison[uid].reset_index(level='Cal Date',inplace=True)\n",
    "    comparison[uid].sort_values(by='Cal Date',inplace=True)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_type(x):\n",
    "    if type(x) is str:\n",
    "        return float(x)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in uids:\n",
    "    comparison[uid] = comparison[uid].applymap(convert_type)\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_the_same(elements):\n",
    "    \"\"\"\n",
    "    This function checks which values in an array are all the same.\n",
    "    \n",
    "    Args:\n",
    "        elements - an array of values\n",
    "    Returns:\n",
    "        error - an array of length (m-1) which checks if\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(elements) < 1:\n",
    "        return True\n",
    "    el = iter(elements)\n",
    "    first = next(el, None)\n",
    "    #check = [element == first for element in el]\n",
    "    error = [np.isclose(element,first) for element in el]\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_cal_error(array):\n",
    "    \"\"\"\n",
    "    This function locates which source file (e.g. xmlcon vs csv vs cal)\n",
    "    have calibration values that are different from the others. It does\n",
    "    NOT identify which is correct, only which is different.\n",
    "    \n",
    "    Args:\n",
    "        array - A numpy array which contains the values for a specific\n",
    "                calibration coefficient for a specific date from all of\n",
    "                the calibration source files\n",
    "    Returns:\n",
    "        dataset - a list containing which calibration sources are different\n",
    "                from the other files\n",
    "        True - if all of the calibration values are the same\n",
    "        False - if the first calibration value is different\n",
    "    \"\"\"\n",
    "    # Call the function to check if there are any differences between each of\n",
    "    # calibration values from the different sheets\n",
    "    error = all_the_same(array)\n",
    "    # If they are all the same, return True\n",
    "    if all(error):\n",
    "        return True\n",
    "    # If there is a mixture of True/False, find the false and return them\n",
    "    elif any(error) == True:\n",
    "        indices = [i+1 for i, j in enumerate(error) if j == False]\n",
    "        dataset = list(array.index[indices])\n",
    "        return dataset\n",
    "    # Last, if all are false, that means the first value \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With all the functions set up, now go through all of the data\n",
    "def search_for_errors(df):\n",
    "    \"\"\"\n",
    "    This function is designed to search through a pandas dataframe\n",
    "    which contains all of the calibration coefficients from all of\n",
    "    the files, and check for differences.\n",
    "    \n",
    "    Args: \n",
    "        df - A dataframe which contains all fo the calibration coefficients\n",
    "        from the asset management csv, qct checkout, and the vendor\n",
    "        files (.cal and .xmlcon)\n",
    "    Returns:\n",
    "        cal_errors - A nested dictionary containing the calibration timestamp, the\n",
    "        relevant calibration coefficient, and which file(s) have the\n",
    "        erroneous calibration file.\n",
    "    \"\"\"\n",
    "    \n",
    "    cal_errors = {}\n",
    "    for date in np.unique(df['Cal Date']):\n",
    "        df2 = df[df['Cal Date'] == date]\n",
    "        wrong_cals = {}\n",
    "        for column in df2.columns.values:\n",
    "            array = df2[column]\n",
    "            array.sort_index()\n",
    "            if array.dtype == 'datetime64[ns]':\n",
    "                pass\n",
    "            else:\n",
    "                error = locate_cal_error(array)\n",
    "                if error == False:\n",
    "                    wrong_cals.update({column:array.index[0]})\n",
    "                elif error == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    wrong_cals.update({column:error})\n",
    "        \n",
    "        if len(wrong_cals) < 1:\n",
    "            cal_errors.update({str(date).split('T')[0]:'No Errors'})\n",
    "        else:\n",
    "            cal_errors.update({str(date).split('T')[0]:wrong_cals})\n",
    "    \n",
    "    return cal_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_errors = {}\n",
    "for uid in uids:\n",
    "    ce = search_for_errors(comparison[uid])\n",
    "    cal_errors.update({uid:ce})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(cal_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame.from_dict({i: cal_errors[i] for i in cal_errors.keys()}, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('CTDBPP_Errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe of the missing files\n",
    "df_missing = pd.DataFrame(index=uids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing['.CAL FILES'] = cal_missing.values()\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing['.XML FILES'] = xml_missing.values()\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing['.QCT FILES'] = qct_missing.values()\n",
    "df_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing.to_csv('CTDBPP_Missing_Files.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which CTDBP-C Calibration files are not correctly named\n",
    "In order to check the calibration values, need to have the correctly named calibration csv files. We can check this by comparison of deployment dates with the CTDBPC calibration dates. This requires loading both the deployment csv and parsing all the file names, flagging the file names THAT MATCH, and then revisiting them in order to correct the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the deployment csvs fo\n",
    "# Parse for all WHOI CG Deployment Sheets based on 'CP' or CG\n",
    "# Easier to check for non-CG \n",
    "deploy_csvs = []\n",
    "for file in os.listdir('../../GitHub/OOI-Integration/asset-management/deployment/'):\n",
    "    if file[0:2] == 'RS' or file[0:2] == 'CE':\n",
    "        pass\n",
    "    elif 'MOAS' in file:\n",
    "        pass\n",
    "    else:\n",
    "        deploy_csvs.append(file)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Deployment History from the WHOI Asset Tracking System\n",
    "CTDBPF_Deploy = CTDBPF['Deployment History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTDBPF_Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the string at the newline to generate a list of deployments for each CTDBP-C\n",
    "CTDBPF_Deploy = CTDBPF['Deployment History'].apply(lambda x: x.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTDBPF_Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List out all the individual deployments\n",
    "deploy_list = []\n",
    "for i in range(0,len(CTDBPF_Deploy)):\n",
    "    for item in CTDBPF_Deploy.iloc[i]:\n",
    "        if '-' in item:\n",
    "            deploy_list.append(item)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I now have a list of the deployments all the CTDBP-Cs were used on.\n",
    "# Now, parse the name of the array to\n",
    "array = list( set( [x.split('-')[0] for x in deploy_list] ) )\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the list of array names, I can now parse the deployment file names to find\n",
    "# the relevant deployment sheets which match where the CTDBP-Cs were deployed\n",
    "deploy_csvs = []\n",
    "for file in os.listdir('../../GitHub/OOI-Integration/asset-management/deployment/'):\n",
    "    if file.split('_')[0] in array:\n",
    "        deploy_csvs.append(file)\n",
    "deploy_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the identified deployment csvs, can now load the deployment csvs into\n",
    "# a pandas dataframe\n",
    "deployments = pd.DataFrame()\n",
    "for file in deploy_csvs:\n",
    "    deployments = deployments.append(pd.read_csv('../../GitHub/OOI-Integration/asset-management/deployment/'+file))\n",
    "deployments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CTDBPF sensor uids\n",
    "sensor_uids = list( set( CTDBPF['UID'] ) )\n",
    "sensor_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find in the deployment spreadsheets the matching entry for the CTDBP-Cs that I'm looking for\n",
    "deployments['CTDBPF'] = deployments['sensor.uid'].apply(lambda x: True if x in sensor_uids else False)\n",
    "deployments = deployments[deployments['CTDBPF'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, parse out the date string in the format of YYYYMMDD from the startDateTime\n",
    "# in order to compare with the date in the calibration file names\n",
    "deploy_dates = deployments['startDateTime'].apply(lambda x: x.replace('-','').split('T')[0])\n",
    "deploy_dates = list(set(deploy_dates))\n",
    "deploy_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_csvs = []\n",
    "for file in os.listdir('../../GitHub/OOI-Integration/asset-management/calibration/CTDBPF/'):\n",
    "    date = file.split('__')[1].split('.')[0]\n",
    "    print(date)\n",
    "    if date in deploy_dates:\n",
    "        cal_csvs = cal_csvs.append(file)\n",
    "print(cal_csvs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_csvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! None of the CTDBP-C have calibration dates which match deployment dates. That is a good sign - it means that the dates in the calibration file name *should* match the calibration dates in the calibration info.\n",
    "\n",
    "However, that is no guarantee that the date in the file name matches the date in the calibration data. This can be check in a future step by comparing the calibration date in the vendor docs, QCT info, and the .cal and .xmlcon file info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, using the \"deploy\" csvs for each node in the various arrays,\n",
    "# need to load into a large pandas dataframe for easy handling\n",
    "import pandas as pd\n",
    "\n",
    "deployments = pd.DataFrame()\n",
    "for file in deploy_csvs:\n",
    "    deployments = deployments.append(pd.read_csv('../GitHub/OOI-Integration/asset-management/deployment/'+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unique deployment dates from the deployment csvs and put into the form of \n",
    "# YYYYMMDD. \n",
    "deploy_dates = deployments['startDateTime'].apply(lambda x: x.split('T')[0].replace('-',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_dates = list(set(deploy_dates))\n",
    "deploy_dates[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deploy_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_files = []\n",
    "for root, dirs, files in os.walk('../GitHub/OOI-Integration/asset-management/calibration/'):\n",
    "    for name in files:\n",
    "        if 'CGINS' in name:\n",
    "            cal_date = name.split('__')[1].split('.')[0]\n",
    "            if cal_date in deploy_dates:\n",
    "                check_files.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, there are a potential 1364 files that we need to check on the\n",
    "# calibration date in the file name, because the parsed date in the \n",
    "# file name matches a deployment date.\n",
    "len(list(set(check_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cool, now save the file to the local working directory\n",
    "with open('calibration_files_to_check.csv','w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(check_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
