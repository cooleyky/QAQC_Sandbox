{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPKIR Checker\n",
    "\n",
    "This notebook is designed to check the SPKIR csv calibration file in pull request. The process I follow is:\n",
    "1. Read in the SPKIR csv from the pull request into a pandas dataframe\n",
    "2. Identify the source file of the calibration coefficients\n",
    "3. Parse the calibration coefficients directly from the source file\n",
    "4. Compare the SPKIR csv from the pull request with the csv parsed from the source file\n",
    "\n",
    "**====================================================================================================================**\n",
    "\n",
    "The first step is to load relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====================================================================================================================**\n",
    "Define the directories where the **csv** file to check is stored, and where the **source** file is stored. Make sure to check the following information on your machine via your terminal first:\n",
    "1. The branch of your local asset-management repository matches the location of the SPKIR cals\n",
    "2. Your local asset-management repository has the requisite **csv** file to check\n",
    "3. You have downloaded the **source** of the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = '/home/andrew/Documents/OOI-CGSN/asset-management/calibration/SPKIRB/'\n",
    "source_dir = '/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/SPKIR/SPKIR_Cal/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the pull request csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGINS-SPKIRB-00302__20151214.csv\n",
      "CGINS-SPKIRB-00302__20170913.csv\n"
     ]
    }
   ],
   "source": [
    "sn = '302'\n",
    "for file in os.listdir(csv_dir):\n",
    "    if sn in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_csv_name = 'CGINS-SPKIRB-00302__20170911.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the pull request csv into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'/home/andrew/Documents/OOI-CGSN/asset-management/calibration/SPKIRB/CGINS-SPKIRB-00302__20170911.csv' does not exist: b'/home/andrew/Documents/OOI-CGSN/asset-management/calibration/SPKIRB/CGINS-SPKIRB-00302__20170911.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-66347e760022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpr_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpr_csv_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpr_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/home/andrew/Documents/OOI-CGSN/asset-management/calibration/SPKIRB/CGINS-SPKIRB-00302__20170911.csv' does not exist: b'/home/andrew/Documents/OOI-CGSN/asset-management/calibration/SPKIRB/CGINS-SPKIRB-00302__20170911.csv'"
     ]
    }
   ],
   "source": [
    "pr_csv = pd.read_csv(csv_dir+pr_csv_name)\n",
    "pr_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrew/Documents/OOI-CGSN/QAQC_Sandbox/Metadata_Review/temp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====================================================================================================================**\n",
    "### Find & Parse the source file\n",
    "Now, we want to find the source file of the calibration coefficients, parse the data using the spkir parser, and read the data into a pandas dataframe. The key pieces of data needed for the parser are:\n",
    "1. Instrument UID: This is needed to initialize the OPTAA parser\n",
    "2. Source file: This is the full path to the source file. Zip files are acceptable input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the source file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(source_dir):\n",
    "    if sn in file:\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file = 'SPKIR-B_OCR-507_SN_302_Calibration_Files_2017-09-11.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/andrew/OS/Users/areed/Documents/Project_Files/Records/Instrument_Records/SPKIR/SPKIR_Cal/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parser with the UID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkir = SPKIRCalibration('CGINS-SPKIRB-00302')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the calibration coefficients from the source file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkir.load_cal(source_dir+source_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the csv to a temporary local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_directory = '/'.join((os.getcwd(),'temp'))\n",
    "# Check if the temp directory exists; if it already does, purge and rewrite\n",
    "if os.path.exists(temp_directory):\n",
    "    shutil.rmtree(temp_directory)\n",
    "    ensure_dir(temp_directory)\n",
    "else:\n",
    "    ensure_dir(temp_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkir.write_csv(temp_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the instrument uid, serial number, and calibration date make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spkir.uid, spkir.serial, spkir.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**====================================================================================================================**\n",
    "### Compare the data sets\n",
    "With the data parsed from the source file and the csv from the pull request, we can now directly compare the between the two datasets and identify any inconsistencies or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_csv = pd.read_csv(temp_directory+'/'+'CGINS-SPKIRB-00302__20170911.csv')\n",
    "source_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat the coefficient value arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_arrays(array):\n",
    "    # First, need to strip extraneous characters from the array\n",
    "    array = array.replace(\"'\",\"\").replace('[','').replace(']','')\n",
    "    # Next, split the array into a list\n",
    "    array = array.split(',')\n",
    "    # Now, need to eliminate any white space surrounding the individual coeffs\n",
    "    array = [num.strip() for num in array]\n",
    "    # Next, float the nums\n",
    "    try:\n",
    "        array = [float(num) for num in array]\n",
    "        # Check if the array is len == 1; if so, can just return the number\n",
    "        if len(array) == 1:\n",
    "            array = array[0]\n",
    "    except:\n",
    "        pass\n",
    "    # Now we are done\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_csv['value'] = source_csv['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_csv['value'] = pr_csv['value'].apply(lambda x: reformat_arrays(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two pandas dataframes for differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_csv == pr_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any (besides notes) return false, iterate through the False position to identify which specific calibration coefficient is incorrect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,m in enumerate(source_csv['value'].iloc[2]):\n",
    "    print(m, pr_csv['value'].iloc[2][n])\n",
    "    print(m == pr_csv['value'].iloc[2][n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**=======================================================================================================================**\n",
    "# Parsing Calibration Coefficients\n",
    "Above, we have worked through identifying and mapping the calibration files and QCT check-ins to the individual instruments through their UIDs and serial numbers. The next step is to open the relevant files and parse out the calibration coefficients. This will require writing a parser for the SPKIR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPKIRCalibration():\n",
    "    # Class that stores calibration values for CTDs.\n",
    "\n",
    "    def __init__(self, uid):\n",
    "        self.serial = None\n",
    "        self.uid = uid\n",
    "        self.date = []\n",
    "        self.coefficients = {\n",
    "            'CC_immersion_factor': [],\n",
    "            'CC_offset': [],\n",
    "            'CC_scale': []\n",
    "        }\n",
    "        self.notes = {\n",
    "            'CC_immersion_factor': '',\n",
    "            'CC_offset': '',\n",
    "            'CC_scale': '',\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def uid(self):\n",
    "        return self._uid\n",
    "\n",
    "    @uid.setter\n",
    "    def uid(self, d):\n",
    "        r = re.compile('.{5}-.{6}-.{5}')\n",
    "        if r.match(d) is not None:\n",
    "            self._uid = d\n",
    "            self.serial = d.split('-')[-1].lstrip('0')\n",
    "        else:\n",
    "            raise Exception(f\"The instrument uid {d} is not a valid uid. Please check.\")\n",
    "            \n",
    "            \n",
    "    def load_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Wrapper function to load all of the calibration coefficients\n",
    "        \n",
    "        Args:\n",
    "            filepath - path to the directory with filename which has the\n",
    "                calibration coefficients to be parsed and loaded\n",
    "        Calls:\n",
    "            open_cal\n",
    "            parse_cal\n",
    "        \"\"\"\n",
    "        \n",
    "        data = self.open_cal(filepath)\n",
    "        \n",
    "        self.parse_cal(data)\n",
    "        \n",
    "        \n",
    "    def open_cal(self, filepath):\n",
    "        \"\"\"\n",
    "        Function that opens and reads in cal file\n",
    "        information for a SPKIR. Zipfiles are acceptable inputs.\n",
    "        \n",
    "        Args:\n",
    "            filepath - path to the directory with filename which has the\n",
    "                calibration coefficients to be parsed and loaded\n",
    "        \n",
    "        Returns:\n",
    "            data - opended calibration file that has been read into \n",
    "                memory but is not parsed\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.endswith('.zip'):\n",
    "            with ZipFile(filepath) as zfile:\n",
    "                # Check if OPTAA has the .dev file\n",
    "                filename = [name for name in zfile.namelist() if name.lower().endswith('.cal')]\n",
    "                \n",
    "                # Get and open the latest calibration file\n",
    "                if len(filename) == 1:\n",
    "                    data = zfile.read(filename[0]).decode('ascii')\n",
    "                    self.source_file(filepath, filename[0])\n",
    "                    \n",
    "                elif len(filename) > 1:\n",
    "                    raise FileExistsError(f\"Multiple .cal files found in {filepath}.\")\n",
    "\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"No .cal file found in {filepath}.\")\n",
    "                        \n",
    "        elif filepath.lower().endswith('.cal'):\n",
    "            with open(filepath) as file:\n",
    "                data = file.read()\n",
    "            self.source_file(filepath, file)\n",
    "          \n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No .cal file found in {filepath}.\")\n",
    "        \n",
    "        return data\n",
    "        \n",
    "        \n",
    "    def parse_cal(self, data):\n",
    "        \"\"\"\n",
    "        Function which parses the calibration data and loads the calibration\n",
    "        coefficients into the object structure.\n",
    "        \n",
    "        Args:\n",
    "            data - calibration data which has been read and loaded into memory\n",
    "        Raises:\n",
    "            ValueError - raised if the serial number parsed from the calibration\n",
    "                data does not match the UID\n",
    "        Returns:\n",
    "            self.coefficients - populated dictionary of calibration coefficient values\n",
    "            self.date - all relevant calibration dates parsed into a dictionary\n",
    "            self.serial - parsed serial data\n",
    "        \"\"\"\n",
    "        \n",
    "        flag = False\n",
    "        for line in data.splitlines():\n",
    "            if line.startswith('#'):\n",
    "                parts = line.split('|')\n",
    "                if len(parts) > 5 and 'Calibration' in parts[-1].strip():\n",
    "                    cal_date = parts[0].replace('#','').strip()\n",
    "                    self.date.append(pd.to_datetime(cal_date).strftime('%Y%m%d'))\n",
    "                    \n",
    "            elif line.startswith('SN'):\n",
    "                parts = line.split()\n",
    "                _, sn, *ignore = parts\n",
    "                sn = sn.lstrip('0')\n",
    "                if self.serial != sn:\n",
    "                    raise ValueError(f'Instrument serial number {sn} does not match UID {self.uid}')\n",
    "                    \n",
    "            elif line.startswith('ED'):\n",
    "                flag = True\n",
    "                \n",
    "            elif flag:\n",
    "                offset, scale, immersion_factor = line.split()\n",
    "                self.coefficients['CC_immersion_factor'].append(float(immersion_factor))\n",
    "                self.coefficients['CC_offset'].append(float(offset))\n",
    "                self.coefficients['CC_scale'].append(float(scale))\n",
    "                flag = False\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "    def source_file(self, filepath):\n",
    "        \"\"\"\n",
    "        Routine which parses out the source file and filename\n",
    "        where the calibration coefficients are sourced from.\n",
    "        \n",
    "        Args:\n",
    "            filepath - path to the directory with filename which has the\n",
    "                calibration coefficients to be parsed and loaded\n",
    "        Returns:\n",
    "            self.source - string which contains the parent file and the\n",
    "                filename of the calibration data source\n",
    "        \"\"\"\n",
    "        \n",
    "        if filepath.lower().endswith('.cal'):\n",
    "            dcn = filepath.split('/')[-2]\n",
    "            filename = filepath.split('/')[-1]\n",
    "        else:\n",
    "            dcn = filepath.split('/')[-1]\n",
    "        \n",
    "        self.source = f'Source file: {dcn} > {filename}'\n",
    "        \n",
    "        \n",
    "    def write_csv(self, outpath):\n",
    "        \"\"\"\n",
    "        This function writes the correctly named csv file for the ctd to the\n",
    "        specified directory.\n",
    "\n",
    "        Args:\n",
    "            outpath - directory path of where to write the csv file\n",
    "        Raises:\n",
    "            ValueError - raised if the CTD object's coefficient dictionary\n",
    "                has not been populated\n",
    "        Returns:\n",
    "            self.to_csv - a csv of the calibration coefficients which is\n",
    "                written to the specified directory from the outpath.\n",
    "        \"\"\"\n",
    "\n",
    "        # Run a check that the coefficients have actually been loaded\n",
    "        if len(self.coefficients.values()) <= 2:\n",
    "            raise ValueError('No calibration coefficients have been loaded.')\n",
    "\n",
    "        # Create a dataframe to write to the csv\n",
    "        data = {\n",
    "            'serial': [self.serial]*len(self.coefficients),\n",
    "            'name': list(self.coefficients.keys()),\n",
    "            'value': list(self.coefficients.values())\n",
    "        }\n",
    "        df = pd.DataFrame().from_dict(data)\n",
    "      \n",
    "        # Now merge the coefficients dataframe with the notes\n",
    "        notes = pd.DataFrame().from_dict({\n",
    "            'name':list(self.notes.keys()),\n",
    "            'notes':list(self.notes.values())\n",
    "        })\n",
    "        df = df.merge(notes, how='outer', left_on='name', right_on='name')\n",
    "            \n",
    "        # Add in the source file\n",
    "        df['notes'].iloc[0] = df['notes'].iloc[0] + ' ' + self.source\n",
    "        \n",
    "        # Sort the data by the coefficient name\n",
    "        df = df.sort_values(by='name')\n",
    "\n",
    "        # Generate the csv names\n",
    "        csv_name = self.uid + '__' + max(self.date) + '.csv'\n",
    "        \n",
    "        # Write the dataframe to a csv file\n",
    "        check = input(f\"Write {csv_name} to {outpath}? [y/n]: \")\n",
    "        # check = 'y'\n",
    "        if check.lower().strip() == 'y':\n",
    "            df.to_csv(outpath+'/'+csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
